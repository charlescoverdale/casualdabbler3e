[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Cookbook for the Casual Dabbler (3rd Edition)",
    "section": "",
    "text": "Introduction\nG’day and welcome to R Cookbook for the Casual Dabbler (3rd edition).\nRCCD 1st edition was originally published in 2020 as a side project during the COVID-19 pandemic in Melbourne.\nAs I wrote in the midst of lock down:\nRCCD2e had surprisingly good (and long-lasting reviews). A lot has continued to change in the R community, and the world more broadly. As such, RCCD3e includes further revisions. Most notably, the book has been rebuilt in Quarto — the successor to RMarkdown — for better formatting, cross-referencing, and long-term maintainability. Additionally:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "R Cookbook for the Casual Dabbler (3rd Edition)",
    "section": "Usage",
    "text": "Usage\nIn each chapter I’ve written up the background, methodology and worked example for a separate piece of analysis.\nMost of this code will not be extraordinary to the seasoned R aficionado. However I find that in classic Pareto style ~20% of my code contributes to the vast majority of my work output. Having this 20% on hand will hopefully be useful to both myself and others.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#additional-resources",
    "href": "index.html#additional-resources",
    "title": "R Cookbook for the Casual Dabbler (3rd Edition)",
    "section": "Additional resources",
    "text": "Additional resources\nThe R community is continually writing new packages and tools. Many of these are covered extensively in various free books available on the bookdown.org website.\nThe rise of LLM’s over the past 2-years has also made it significantly easier to find, refine, and expand on R code. RCCD2e includes optimized (and better formated) code which has gone through the scrutiny of many of the different LLM’s.\nI encourage users to paste in code snippets to language models (or use Cursor) for deeper explanations or alternate examples.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#limitations",
    "href": "index.html#limitations",
    "title": "R Cookbook for the Casual Dabbler (3rd Edition)",
    "section": "Limitations",
    "text": "Limitations\nIf you find a bug (along with spelling errors etc) please email me at charlesfcoverdale@gmail.com with the subject line ‘RCCD3e’.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "R Cookbook for the Casual Dabbler (3rd Edition)",
    "section": "About the author",
    "text": "About the author\nCharles Coverdale is an economist working across London and Melbourne. He is passionate about economics, climate science, and building talented teams. You can get in touch with Charles on twitter to hear more about his current projects.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-making-maps-beautiful.html",
    "href": "02-making-maps-beautiful.html",
    "title": "Making maps beautiful",
    "section": "",
    "text": "Why use a map\nMaps are a great way to communicate data.\nThey’re easily understandable, flexible, and more intuitive than a chart. There’s been numerous studies showing that the average reader often struggles to interpret the units on a y-axis, let alone understand trends in scatter or line graphs.\nMaking maps in R takes some initial investment. However once you have some code you know and understand, spinning up new pieces of analysis can happen in minutes, rather than hours or days.\nThe aim of this chapter is to get you from ‘I can make a map in R’ to something more like ‘I can conduct spatial analysis and produce a visual which is ready for publication’.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Making maps beautiful</span>"
    ]
  },
  {
    "objectID": "02-making-maps-beautiful.html#getting-started",
    "href": "02-making-maps-beautiful.html#getting-started",
    "title": "Making maps beautiful",
    "section": "Getting started",
    "text": "Getting started\nFirst up, we need to load a bunch of mapping packages. The tidyverse package is a classic for just about everything data manipulation, while, the sf and ggspatial packages are essential for making maps.\n\n# Load required packages\nlibrary(tidyverse) # Includes ggplot2, dplyr, tidyr, readxl, purrr\nlibrary(ggmap)\nlibrary(sf)\nlibrary(ggspatial)\nlibrary(rlang)\nlibrary(broom)\nlibrary(Census2016)\nlibrary(strayr)\nlibrary(officer)\n\nWill Mackey’s absmapsdata package contains all the ABS’ ASGS shapefiles. The data is now also callable through the (thoroughly updated and expanded) strayr package. For example:\nstrayr::read_absmap(\"sa12016\")\nTo get a basic demographic map up and running, we will splice together the shapefile (in this case the SA2 map of Austrlaia) and some data from the 2016 Australian Census.\nHugh Parsonage put together a fantastic packaged called Census2016 which makes downloading this data in a clean format easy.\n\n# Get the shapefile form the absmapsdata package (predefined in the list above)\n\n# Get the 2016 census dataset\ncensus2016_wide &lt;- Census2016_wide_by_SA2_year\n\n# Select the key demographic columns from the census data (i.e. the first 8 variables)\ncensus_short &lt;- census2016_wide[, 1:8]\n\n# Filter for a single year\ncensus_short_2016 &lt;- census_short %&gt;%\n  filter(year == 2016)\n\n# Use the inner_join function to get the shapefile and census wide data into a single df for analysis / visualisation\nSA2_shp_census_2016 &lt;- inner_join(strayr::read_absmap(\"sa22016\"), census_short_2016,\n  by = c(\"sa2_name_2016\" = \"sa2_name\")\n)\n\n# Plot a map that uses census data\nmap1 &lt;- ggplot() +\n  geom_sf(data = SA2_shp_census_2016, aes(fill = median_age)) +\n  ggtitle(\"Australian median age (SA2)\") +\n  xlab(\"Longitude\") +\n  ylab(\"Latitude\") +\n  theme_bw() +\n  theme(legend.position = \"right\")\n\nmap1\n\n\n\n\n\n\n\n\nThere we go! A map. This looks ‘okay’… but it can be much better.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Making maps beautiful</span>"
    ]
  },
  {
    "objectID": "02-making-maps-beautiful.html#from-okay-to-good",
    "href": "02-making-maps-beautiful.html#from-okay-to-good",
    "title": "Making maps beautiful",
    "section": "From okay to good",
    "text": "From okay to good\nHeat maps don’t really show too much interesting data on such a large scale, so let’s filter down to Greater Melbourne.\nSeeing we have a bunch of census data in our dataframe, we can also do some basic analysis (e.g. population density).\n\n# As a bit of an added extra, we can create a new population density column\nSA2_shp_census_2016 &lt;- SA2_shp_census_2016 %&gt;%\n  mutate(pop_density = persons / areasqkm_2016)\n\n# Filter for Greater Melbourne\nMEL_SA2_shp_census_2016 &lt;- SA2_shp_census_2016 %&gt;%\n  filter(gcc_name_2016 == \"Greater Melbourne\")\n\n# Plot the new map just for Greater Melbourne\nmap2 &lt;- ggplot() +\n  geom_sf(data = MEL_SA2_shp_census_2016, aes(fill = median_age, border = NA)) +\n  ggtitle(\"Median age in Melbourne (SA2)\") +\n  xlab(\"Longitude\") +\n  ylab(\"Latitude\") +\n  theme_bw() +\n  theme(legend.position = \"right\")\nmap2\n\n\n\n\n\n\n\n\nMuch better. We can start to see some trends in this map. It looks like younger people tend to live closer to the city center. This seems logical.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Making maps beautiful</span>"
    ]
  },
  {
    "objectID": "02-making-maps-beautiful.html#from-good-to-great",
    "href": "02-making-maps-beautiful.html#from-good-to-great",
    "title": "Making maps beautiful",
    "section": "From good to great",
    "text": "From good to great\nThe map above is a good start! However, how do we turn this from something ‘good’, into something that is 100% ready to share?\nWe see our ‘ink to chart ratio’ (i.e. the amount of non-data stuff that is on the page) is still pretty high. Is the latitude of Melbourne useful for this analysis…? Not really. Let’s get rid of it and the axis labels. A few lines of code adjusting the axis, titles, and theme of the plot will go a long way. Because my geography Professor drilled it into me, I will also add a low-key scale bar.\n\nmap3 &lt;- ggplot() +\n  geom_sf(data = MEL_SA2_shp_census_2016, aes(fill = median_age)) +\n  labs(\n    title = \"Melbourne's youth tend to live closer to the city centre\",\n    subtitle = \"Analysis from the 2016 census\",\n    caption = \"Data: Australian Bureau of Statistics 2016\",\n    x = \"\",\n    y = \"\",\n    fill = \"Median age\"\n  ) +\n  ggspatial::annotation_scale(location = \"br\") +\n  theme_minimal() +\n  theme(axis.ticks.x = element_blank(), axis.text.x = element_blank()) +\n  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme(legend.position = \"right\") +\n  theme(plot.title = element_text(face = \"bold\", size = 12)) +\n  theme(plot.subtitle = element_text(size = 11)) +\n  theme(plot.caption = element_text(size = 8))\n\nmap3",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Making maps beautiful</span>"
    ]
  },
  {
    "objectID": "02-making-maps-beautiful.html#from-great-to-fantastic",
    "href": "02-making-maps-beautiful.html#from-great-to-fantastic",
    "title": "Making maps beautiful",
    "section": "From great to fantastic",
    "text": "From great to fantastic\nThe above is perfectly reasonable and looks professionally designed. However, this is where we can get really special.\nLet’s add a custom colour scheme, drop the boundary edges for the SA2’s, and add in a dot and label for Melbourne CBD.\n\n# Add in a point for the Melbourne CBD\nMEL_location &lt;- data.frame(\n  town_name = c(\"Melbourne\"),\n  x = c(144.9631),\n  y = c(-37.8136)\n)\n\nmap4 &lt;- ggplot() +\n  geom_sf(data = MEL_SA2_shp_census_2016, aes(fill = median_age), color = NA) +\n  geom_point(data = MEL_location, aes(x = x, y = y), size = 2, color = \"black\") +\n  labs(\n    title = \"Melbourne's youth tend to live closer to the city centre\",\n    subtitle = \"Analysis from the 2016 census\",\n    caption = \"Data: Australian Bureau of Statistics 2016\",\n    x = \"\",\n    y = \"\",\n    fill = \"Median age\"\n  ) +\n  scale_fill_steps(low = \"#E2E0EB\", high = \"#3C33FE\") +\n  annotate(\n    geom = \"curve\",\n    x = 144.9631,\n    y = -37.8136,\n    xend = 144.9,\n    yend = -38.05,\n    curvature = 0.5,\n    arrow = arrow(length = unit(2, \"mm\"))\n  ) +\n  annotate(geom = \"text\", x = 144.76, y = -38.1, label = \"Melbourne CBD\") +\n  ggspatial::annotation_scale(location = \"br\") +\n  theme_minimal() +\n  theme(axis.ticks.x = element_blank(), axis.text.x = element_blank()) +\n  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme(legend.position = \"right\") +\n  theme(plot.title = element_text(face = \"bold\", size = 12)) +\n  theme(plot.subtitle = element_text(size = 11)) +\n  theme(plot.caption = element_text(size = 8))\n\nmap4\n\n\n\n\n\n\n\n\nNow we’re talking. A ‘client-ready’ looking map that can be added to a report, presentation, or with a few tweaks - a digital dashboard.\nMake sure to export the map as a high quality PNG using the ggplot2:ggsave() function.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Making maps beautiful</span>"
    ]
  },
  {
    "objectID": "03-charts.html",
    "href": "03-charts.html",
    "title": "Charts",
    "section": "",
    "text": "Packages matter\nThere’s exceptional resources online for using the ggplot2 package and the broader tidyverse suite to create production ready charts.\nThe R Graph Gallery is a great place to start, as is the visual storytelling blogs of The Economist and the BBC.\nThis chapter contains the code for some of my most used charts and visualization techniques.\n# Load core packages\nlibrary(tidyverse) # Includes ggplot2, dplyr, tidyr, purrr, and readr\nlibrary(lubridate)\nlibrary(readxl)\nlibrary(scales)\n\n# Load additional visualization libraries\nlibrary(ggridges)\nlibrary(ggrepel)\nlibrary(viridis)\nlibrary(patchwork)\n\n# Load additional data manipulation libraries\nlibrary(reshape2)\nlibrary(gapminder)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Charts</span>"
    ]
  },
  {
    "objectID": "03-charts.html#make-the-data-tidy",
    "href": "03-charts.html#make-the-data-tidy",
    "title": "Charts",
    "section": "Make the data tidy",
    "text": "Make the data tidy\nBefore making a chart ensure the data is “tidy” - meaning there is a new row for every changed variable. It also doesn’t hurt to remove NA’s for consistency (particularly in time series).\n\n# Read in data\nurl &lt;- \"https://raw.githubusercontent.com/charlescoverdale/ggridges/master/2019_MEL_max_temp_daily.xlsx\"\n\n# Read in with read.xlsx\nMEL_temp_daily &lt;- openxlsx::read.xlsx(url)\n\n# Remove last 2 characters to just be left with the day number\nMEL_temp_daily$Day &lt;- substr(MEL_temp_daily$Day, 1, nchar(MEL_temp_daily$Day) - 2)\n\n# Make a wide format long using the gather function\nMEL_temp_daily &lt;- MEL_temp_daily %&gt;%\n  gather(Month, Temp, Jan:Dec)\n\nMEL_temp_daily$Month &lt;- factor(MEL_temp_daily$Month, levels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"))\n\n# Add in a year\nMEL_temp_daily[\"Year\"] &lt;- 2019\n\n# Reorder\nMEL_temp_daily &lt;- MEL_temp_daily[, c(1, 2, 4, 3)]\n\n# Make a single data field using lubridate\nMEL_temp_daily &lt;- MEL_temp_daily %&gt;% mutate(Date = make_date(Year, Month, Day))\n\n# Drop the original date columns\nMEL_temp_daily &lt;- MEL_temp_daily %&gt;%\n  dplyr::select(Date, Temp) %&gt;%\n  drop_na()\n\n# Add on a 7-day rolling average\nMEL_temp_daily &lt;- MEL_temp_daily %&gt;% dplyr::mutate(\n  Seven_day_rolling =\n    zoo::rollmean(Temp, k = 7, fill = NA),\n  Mean = mean(Temp)\n)\n# Drop NA's\n# MEL_temp_daily &lt;- MEL_temp_daily %&gt;% drop_na()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Charts</span>"
    ]
  },
  {
    "objectID": "03-charts.html#line-plot",
    "href": "03-charts.html#line-plot",
    "title": "Charts",
    "section": "Line plot",
    "text": "Line plot\n\nplot_MEL_temp &lt;- ggplot(MEL_temp_daily, aes(x = Date)) +\n  geom_line(aes(y = Temp), col = \"blue\") +\n  geom_line(aes(y = Mean), col = \"orange\") +\n  labs(\n    title = \"Hot in the summer and cool in the winter\",\n    subtitle = \"Analysing temperature in Melbourne\",\n    caption = \"Data: Bureau of Meteorology 2019\",\n    x = \"\",\n    y = \"\"\n  ) +\n  scale_x_date(\n    date_breaks = \"1 month\",\n    date_labels = \"%b\",\n    limits = as.Date(c(\"2019-01-01\", \"2019-12-14\"))\n  ) +\n  scale_y_continuous(labels = unit_format(unit = \"\\u00b0C\", sep = \"\")) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(face = \"bold\", size = 12),\n    plot.subtitle = element_text(size = 11),\n    plot.caption = element_text(size = 8),\n    axis.text = element_text(size = 8),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    axis.line.x = element_line(colour = \"black\", size = 0.4),\n    axis.ticks.x = element_line(colour = \"black\", size = 0.4)\n  ) +\n  annotate(\n    geom = \"curve\",\n    x = as.Date(\"2019-08-01\"), y = 23,\n    xend = as.Date(\"2019-08-01\"), yend = 17,\n    curvature = -0.5, arrow = arrow(length = unit(2, \"mm\"))\n  ) +\n  annotate(\n    geom = \"text\",\n    x = as.Date(\"2019-07-15\"), y = 25,\n    label = \"Below 20°C all winter\"\n  )\n\nplot_MEL_temp",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Charts</span>"
    ]
  },
  {
    "objectID": "03-charts.html#scatter-and-trend-plot",
    "href": "03-charts.html#scatter-and-trend-plot",
    "title": "Charts",
    "section": "Scatter and trend plot",
    "text": "Scatter and trend plot\n\nMEL_temp_Jan &lt;- MEL_temp_daily %&gt;% filter(Date &lt; as.Date(\"2019-01-31\"))\n\nggplot(MEL_temp_Jan, aes(x = Date, y = Temp)) +\n  geom_point(col = \"purple\", alpha = 0.4) +\n  geom_smooth(col = \"purple\", fill = \"purple\", alpha = 0.1, method = \"lm\") +\n  labs(\n    title = \"January is a hot one\",\n    subtitle = \"Analysing temperature in Melbourne\",\n    caption = \"Data: Bureau of Meteorology 2019\",\n    x = \"\",\n    y = \"Temperature °C\"\n  ) +\n  scale_x_date(\n    date_breaks = \"1 week\",\n    date_labels = \"%d-%b\",\n    limits = as.Date(c(\"2019-01-01\", \"2019-01-31\")),\n    expand = c(0, 0)\n  ) +\n  geom_hline(yintercept = 45, colour = \"black\", size = 0.4) +\n  annotate(\n    geom = \"curve\",\n    x = as.Date(\"2019-01-22\"), y = 37.5,\n    xend = as.Date(\"2019-01-25\"), yend = 42,\n    curvature = 0.5,\n    col = \"#575757\",\n    arrow = arrow(length = unit(2, \"mm\"))\n  ) +\n  annotate(\n    geom = \"text\",\n    x = as.Date(\"2019-01-16\"), y = 37.5,\n    label = \"January saw some extreme temperatures\",\n    size = 3.2, col = \"#575757\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 12),\n    plot.subtitle = element_text(size = 11),\n    plot.caption = element_text(size = 8),\n    axis.text = element_text(size = 8),\n    axis.title.y = element_text(size = 9, margin = ggplot2::margin(r = 10)),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    axis.line.x = element_line(colour = \"black\", size = 0.4),\n    axis.ticks.x = element_line(colour = \"black\", size = 0.4)\n  )",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Charts</span>"
    ]
  },
  {
    "objectID": "03-charts.html#shading-areas-on-plots",
    "href": "03-charts.html#shading-areas-on-plots",
    "title": "Charts",
    "section": "Shading areas on plots",
    "text": "Shading areas on plots\nAdding shading behind a plot area is simple using geom_rect.\nAdding shading under a particular model line? A little trickier. See both example below.\n\n# Example 1: Custom y-axis threshold shading\nthreshold &lt;- 20\n\nggplot(mtcars, aes(x = hp, y = mpg)) +\n  geom_point() +\n  geom_hline(yintercept = threshold) +\n\n  # Shade areas below and above threshold\n  geom_rect(\n    xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = threshold,\n    fill = \"blue\", alpha = 0.2\n  ) +\n  geom_rect(\n    xmin = -Inf, xmax = Inf, ymin = threshold, ymax = Inf,\n    fill = \"red\", alpha = 0.2\n  )\n\n\n\n\n\n\n\n# Example 2: Model line with shaded areas\nmodel &lt;- lm(mpg ~ log(hp), data = mtcars)\n\n# Generate predicted values for plotting\ndf_line &lt;- data.frame(hp = seq(min(mtcars$hp), max(mtcars$hp), by = 1))\ndf_line$mpg &lt;- predict(model, newdata = df_line)\n\n# Define polygons above and below the model line\ndf_poly_under &lt;- bind_rows(df_line, tibble(hp = c(max(df_line$hp), min(df_line$hp)), mpg = c(-Inf, -Inf)))\ndf_poly_above &lt;- bind_rows(df_line, tibble(hp = c(max(df_line$hp), min(df_line$hp)), mpg = c(Inf, Inf)))\n\n# Plot the data, model line, and shaded areas\nggplot(mtcars, aes(x = hp, y = mpg)) +\n  geom_point(color = \"grey\", alpha = 0.5) +\n  geom_line(data = df_line, aes(x = hp, y = mpg), color = \"black\") +\n\n  # Shaded areas\n  geom_polygon(data = df_poly_under, aes(x = hp, y = mpg), fill = \"blue\", alpha = 0.2) +\n  geom_polygon(data = df_poly_above, aes(x = hp, y = mpg), fill = \"red\", alpha = 0.2) +\n  scale_x_continuous(expand = c(0, 0)) +\n  labs(\n    title = \"Look at that snazzy red/blue shaded area\",\n    subtitle = \"Subtitle goes here\",\n    caption = \"Data: Made up from scratch\",\n    x = \"\",\n    y = \"\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank()\n  )",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Charts</span>"
    ]
  },
  {
    "objectID": "03-charts.html#bar-chart",
    "href": "03-charts.html#bar-chart",
    "title": "Charts",
    "section": "Bar chart",
    "text": "Bar chart\n\n# Create data frame directly\nbar_data_single &lt;- data.frame(\n  Year = c(\"2018\", \"2019\", \"2020\", \"2021\"),\n  Value = c(1000000, 3000000, 2000000, 5000000)\n)\n\nggplot(bar_data_single, aes(x = Year, y = Value)) +\n  geom_bar(stat = \"identity\", fill = \"blue\", width = 0.8) +\n\n  # Labels in the middle of the bars\n  geom_text(aes(y = Value / 2, label = scales::dollar(Value, scale = 1 / 1e6, suffix = \"m\")),\n    size = 5, color = \"white\", fontface = \"bold\"\n  ) +\n  labs(\n    title = \"Bar chart example\",\n    subtitle = \"Subtitle goes here\",\n    caption = \"Data: Made up from scratch\",\n    x = \"\",\n    y = \"\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 12),\n    plot.subtitle = element_text(size = 11),\n    plot.caption = element_text(size = 12),\n    axis.text = element_text(size = 12),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  )\n\n\n\n\n\n\n\n# Save the plot\n# ggsave(\"test.png\", width = 10, height = 10, units = \"cm\", dpi = 600)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Charts</span>"
    ]
  },
  {
    "objectID": "03-charts.html#stacked-bar-chart",
    "href": "03-charts.html#stacked-bar-chart",
    "title": "Charts",
    "section": "Stacked bar chart",
    "text": "Stacked bar chart\n\nYear &lt;- c(\"2019\", \"2019\", \"2019\", \"2019\", \"2020\", \"2020\", \"2020\", \"2020\")\n\nQuarter &lt;- c(\"Q1\", \"Q2\", \"Q3\", \"Q4\", \"Q1\", \"Q2\", \"Q3\", \"Q4\")\n\nValue &lt;- (c(100, 300, 200, 500, 400, 700, 200, 300))\n\nbar_data &lt;- (cbind(Year, Quarter, Value))\n\nbar_data &lt;- as.data.frame(bar_data)\n\nbar_data$Value &lt;- as.integer(bar_data$Value)\n\nbar_data_totals &lt;- bar_data %&gt;%\n  dplyr::group_by(Year) %&gt;%\n  dplyr::summarise(Total = sum(Value))\n\n\nggplot(bar_data, aes(x = Year, y = Value, fill = (Quarter), label = Value)) +\n  geom_bar(position = position_stack(reverse = TRUE), stat = \"identity\") +\n  geom_text(\n    size = 4,\n    col = \"white\",\n    fontface = \"bold\",\n    position = position_stack(reverse = TRUE, vjust = 0.5),\n    label = scales::dollar(Value)\n  ) +\n  geom_text(\n    aes(Year, Total,\n      label = scales::dollar(Total),\n      fill = NULL,\n      vjust = -0.5\n    ),\n    fontface = \"bold\",\n    size = 4,\n    data = bar_data_totals\n  ) +\n  scale_fill_brewer(palette = \"Blues\") +\n  labs(\n    title = \"Bar chart example\",\n    subtitle = \"Subtitle goes here\",\n    caption = \"Data: Made up from scratch\",\n    x = \"\",\n    y = \"Units\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  theme(legend.title = element_blank()) +\n  theme(plot.title = element_text(face = \"bold\", size = 12)) +\n  theme(plot.subtitle = element_text(size = 10)) +\n  theme(plot.caption = element_text(size = 8)) +\n  theme(axis.text = element_text(size = 10)) +\n  theme(panel.grid.minor = element_blank()) +\n  theme(panel.grid.major.x = element_blank()) +\n  theme(panel.grid.major.y = element_blank()) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, 1800)) +\n  theme(\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  )",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Charts</span>"
    ]
  },
  {
    "objectID": "03-charts.html#histogram",
    "href": "03-charts.html#histogram",
    "title": "Charts",
    "section": "Histogram",
    "text": "Histogram\nAka. a bar chart for a continuous variable where the bars are touching. Useful to show distribution of time series or ordinal variables.\n\nc(\"0-20\", \"20-40\", \"40-60\", \"60-80\", \"80+\")\n\n[1] \"0-20\"  \"20-40\" \"40-60\" \"60-80\" \"80+\"  \n\n# Create a data set\nhist_data &lt;- data.frame(X1 = sample(0:100, 100, rep = TRUE))\n\nggplot(hist_data) +\n  geom_histogram(aes(x = X1), binwidth = 5, fill = \"blue\", alpha = 0.5) +\n  geom_vline(xintercept = c(50, 75, 95), yintercept = 0, linetype = \"longdash\", col = \"orange\") +\n  labs(\n    title = \"Histogram example\",\n    subtitle = \"Facet wraps are looking good\",\n    caption = \"Data: Made up from scratch\",\n    x = \"\",\n    y = \"\"\n  ) +\n  theme_minimal() +\n  theme(panel.spacing.x = unit(10, \"mm\")) +\n  theme(legend.position = \"none\") +\n  theme(plot.title = element_text(face = \"bold\", size = 12)) +\n  theme(plot.subtitle = element_text(size = 11)) +\n  theme(plot.caption = element_text(size = 8)) +\n  theme(axis.text = element_text(size = 9)) +\n  theme(panel.grid.minor = element_blank()) +\n  theme(panel.grid.major.x = element_blank()) +\n  theme(plot.subtitle = element_text(margin = ggplot2::margin(0, 0, 15, 0))) +\n  theme(plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), \"cm\"))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Charts</span>"
    ]
  },
  {
    "objectID": "03-charts.html#ridge-chart",
    "href": "03-charts.html#ridge-chart",
    "title": "Charts",
    "section": "Ridge chart",
    "text": "Ridge chart\nHandy when working with climate variables. Particularly useful at showing the difference in range of multiples series (e.g. temperature by month).\n\n# Import data\nurl &lt;- \"https://raw.githubusercontent.com/charlescoverdale/ggridges/master/2019_MEL_max_temp_daily.xlsx\"\n\nMEL_temp_daily &lt;- openxlsx::read.xlsx(url)\n\n# Remove last 2 characters to just be left with the day number\nMEL_temp_daily$Day &lt;- substr(MEL_temp_daily$Day, 1, nchar(MEL_temp_daily$Day) - 2)\n\n# Make a wide format long using the gather function\nMEL_temp_daily &lt;- MEL_temp_daily %&gt;%\n  gather(Month, Temp, Jan:Dec)\n\nMEL_temp_daily$Month &lt;- factor(MEL_temp_daily$Month, levels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"))\n\n# Plot\nggplot(\n  MEL_temp_daily,\n  aes(x = Temp, y = Month, fill = stat(x))\n) +\n  geom_density_ridges_gradient(\n    scale = 2,\n    size = 0.3,\n    rel_min_height = 0.01,\n    gradient_lwd = 1.\n  ) +\n  scale_y_discrete(limits = unique(rev(MEL_temp_daily$Month))) +\n  scale_fill_viridis_c(name = \"°C\", option = \"C\") +\n  labs(\n    title = \"Melbourne temperature profile\",\n    subtitle = \"Daily maximum temperature recorded in Melbourne in 2019\",\n    caption = \"Data: Bureau of Meteorology 2020\"\n  ) +\n  xlab(\" \") +\n  ylab(\" \") +\n  theme_ridges(font_size = 13, grid = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Charts</span>"
    ]
  },
  {
    "objectID": "03-charts.html#bbc-style-bar-charts-categorical",
    "href": "03-charts.html#bbc-style-bar-charts-categorical",
    "title": "Charts",
    "section": "BBC style: Bar charts (categorical)",
    "text": "BBC style: Bar charts (categorical)\n\n# devtools::install_github('bbc/bbplot')\nlibrary(gapminder)\nlibrary(bbplot)\n\n# Prepare data\nbar_df &lt;- gapminder %&gt;%\n  filter(year == 2007 & continent == \"Africa\") %&gt;%\n  arrange(desc(lifeExp)) %&gt;%\n  head(5)\n\n# Make plot\nbars &lt;- ggplot(bar_df, aes(x = reorder(country, lifeExp), y = lifeExp, fill = country == \"Mauritius\")) +\n  geom_bar(stat = \"identity\", position = \"identity\") +\n  geom_hline(yintercept = 0, size = 1, colour = \"#333333\") +\n  scale_fill_manual(values = c(\"TRUE\" = \"#1380A1\", \"FALSE\" = \"#dddddd\")) +\n  labs(\n    title = \"Mauritius has the highest life expectancy\",\n    subtitle = \"Top 5 African countries by life expectancy, 2007\"\n  ) +\n  coord_flip() +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_line(color = \"#cbcbcb\"),\n    panel.grid.major.y = element_blank(),\n    legend.position = \"none\"\n  )\n\n# Add labels\nlabelled_bars &lt;- bars +\n  geom_label(aes(label = round(lifeExp, 0)),\n    hjust = 1, vjust = 0.5, colour = \"white\",\n    fill = \"black\", label.size = 0.2,\n    family = \"Helvetica\", size = 6\n  )\n\nlabelled_bars",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Charts</span>"
    ]
  },
  {
    "objectID": "03-charts.html#bbc-style-dumbbell-charts",
    "href": "03-charts.html#bbc-style-dumbbell-charts",
    "title": "Charts",
    "section": "BBC style: Dumbbell charts",
    "text": "BBC style: Dumbbell charts\nDumbbell charts are handy instead of using clustered column charts with janky thinkcell labels and arrows to show the difference between the columns. Note it relies on having a 4 variable input (variable_name, value1, value2, and gap).\nThe geom_dumbellfunction lives inside the ggalt package rather than the standard ggplot2.\n\nlibrary(ggalt) # For geom_dumbbell\nlibrary(gapminder)\nlibrary(bbplot) # Uncomment if using bbc_style()\n\n# Prepare data\ndumbbell_df &lt;- gapminder %&gt;%\n  filter(year %in% c(1967, 2007)) %&gt;%\n  select(country, year, lifeExp) %&gt;%\n  pivot_wider(names_from = year, values_from = lifeExp) %&gt;%\n  mutate(gap = `2007` - `1967`) %&gt;%\n  arrange(desc(gap)) %&gt;%\n  head(10)\n\n# Make plot\nggplot(dumbbell_df, aes(x = `1967`, xend = `2007`, y = reorder(country, gap), group = country)) +\n  geom_dumbbell(\n    colour = \"#dddddd\",\n    size = 3,\n    colour_x = \"#FAAB18\",\n    colour_xend = \"#1380A1\"\n  ) +\n  labs(\n    title = \"We're Living Longer\",\n    subtitle = \"Top 10 biggest increases in life expectancy (1967-2007)\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank()\n  )",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Charts</span>"
    ]
  },
  {
    "objectID": "03-charts.html#facet-wraps",
    "href": "03-charts.html#facet-wraps",
    "title": "Charts",
    "section": "Facet wraps",
    "text": "Facet wraps\nHandy rather than showing multiple lines on the same chart.\nTop tips: facet_wrap()dataframes need to be in long form in order to be manipulated easily.\nIt also helps to add on separate columns for the start and end values (if you want to add data point labels).\n\n# Create a data set\nYear &lt;- c(\"2018\", \"2019\", \"2020\", \"2021\")\n\nQLD &lt;- (c(500, 300, 500, 600))\n\nNSW &lt;- (c(200, 400, 500, 700))\n\nVIC &lt;- (c(300, 400, 500, 600))\n\n# Combine the columns into a single dataframe\nfacet_data &lt;- (cbind(Year, QLD, NSW, VIC))\nfacet_data &lt;- as.data.frame(facet_data)\n\n# Change formats to integers\nfacet_data$QLD &lt;- as.integer(facet_data$QLD)\nfacet_data$NSW &lt;- as.integer(facet_data$NSW)\nfacet_data$VIC &lt;- as.integer(facet_data$VIC)\n\n# Make the wide data long\nfacet_data_long &lt;- pivot_longer(facet_data, !Year, names_to = \"State\", values_to = \"Value\")\n\nfacet_data_long &lt;- facet_data_long %&gt;%\n  dplyr::mutate(\n    start_label =\n      if_else(Year == min(Year),\n        as.integer(Value), NA_integer_\n      )\n  )\n\nfacet_data_long &lt;- facet_data_long %&gt;%\n  dplyr::mutate(\n    end_label =\n      if_else(Year == max(Year),\n        as.integer(Value), NA_integer_\n      )\n  )\n\n\n# Make the base line chart\nbase_chart &lt;- ggplot() +\n  geom_line(\n    data = facet_data_long,\n    aes(\n      x = Year,\n      y = Value,\n      group = State,\n      colour = State\n    )\n  ) +\n  geom_point(\n    data = facet_data_long,\n    aes(\n      x = Year,\n      y = Value,\n      group = State,\n      colour = State\n    )\n  ) +\n  ggrepel::geom_text_repel(\n    data = facet_data_long,\n    aes(\n      x = Year,\n      y = Value,\n      label = end_label\n    ),\n    color = \"black\",\n    nudge_y = -10, size = 3\n  ) +\n  ggrepel::geom_text_repel(\n    data = facet_data_long,\n    aes(\n      x = Year,\n      y = Value,\n      label = start_label\n    ),\n    color = \"black\",\n    nudge_y = 10, size = 3\n  )\n\n\nbase_chart +\n\n  scale_x_discrete(\n    breaks = seq(2018, 2021, 1),\n    labels = c(\"2018\", \"19\", \"20\", \"21\")\n  ) +\n\n  facet_wrap(State ~ .) +\n  # To control the grid arrangement, we can add in customer dimensions\n  # ncol = 2, nrow=2) +\n\n  labs(\n    title = \"State by state comparison\",\n    subtitle = \"Facet wraps are looking good\",\n    caption = \"Data: Made up from scratch\",\n    x = \"\",\n    y = \"\"\n  ) +\n\n\n  theme_minimal() +\n\n  theme(strip.text.x = element_text(size = 9, face = \"bold\")) +\n\n  theme(panel.spacing.x = unit(10, \"mm\")) +\n\n  theme(legend.position = \"none\") +\n\n  theme(plot.title = element_text(face = \"bold\", size = 12)) +\n  theme(plot.subtitle = element_text(size = 11)) +\n  theme(plot.caption = element_text(size = 8)) +\n\n  theme(axis.text = element_text(size = 9)) +\n  theme(panel.grid.minor = element_blank()) +\n  theme(panel.grid.major.x = element_blank()) +\n\n  theme(plot.subtitle = element_text(margin = ggplot2::margin(0, 0, 15, 0))) +\n\n  theme(plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), \"cm\"))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Charts</span>"
    ]
  },
  {
    "objectID": "03-charts.html#pie-chart",
    "href": "03-charts.html#pie-chart",
    "title": "Charts",
    "section": "Pie chart",
    "text": "Pie chart\nThese should be used sparingly… but they are handy for showing proportions when the proportion of the whole is paramount (e.g. 45%) - rather than the proportion in relation to another data point (e.g. 16% one year vs 18% the next).\n\n# Create Data\npie_data &lt;- data.frame(\n  group = LETTERS[1:5],\n  value = c(13, 7, 9, 21, 2)\n)\n\n# Compute the position of labels\npie_data &lt;- pie_data %&gt;%\n  arrange(desc(group)) %&gt;%\n  mutate(proportion = value / sum(pie_data$value) * 100) %&gt;%\n  mutate(ypos = cumsum(proportion) - 0.5 * proportion)\n\n# Basic piechart\nggplot(pie_data, aes(x = \"\", y = proportion, fill = group)) +\n  geom_bar(stat = \"identity\") +\n  coord_polar(\"y\", start = 0) +\n  theme_void() +\n  geom_text(\n    aes(\n      y = ypos,\n      label = paste(round(proportion, digits = 0), \"%\", sep = \"\"), x = 1.25\n    ),\n    color = \"white\",\n    size = 4\n  ) +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(\n    title = \"Use pie charts sparingly\",\n    subtitle = \"Subtitle goes here\",\n    caption = \"\",\n    x = \"\",\n    y = \"\"\n  ) +\n  theme(legend.position = \"bottom\") +\n  theme(legend.title = element_blank()) +\n  theme(plot.title = element_text(face = \"bold\", size = 12)) +\n  theme(plot.subtitle = element_text(size = 11)) +\n  theme(plot.caption = element_text(size = 8)) +\n  theme(plot.subtitle = element_text(margin = ggplot2::margin(0, 0, 5, 0))) +\n  theme(plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), \"cm\"))\n\n\n\n\n\n\n\n# ggsave(plot=last_plot(),\n#      width=10,\n#      height=10,\n#      units=\"cm\",\n#      dpi = 600,\n#      filename = \"/Users/charlescoverdale/Desktop/pietest.png\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Charts</span>"
    ]
  },
  {
    "objectID": "03-charts.html#patchwork",
    "href": "03-charts.html#patchwork",
    "title": "Charts",
    "section": "Patchwork",
    "text": "Patchwork\nPatchwork is a nifty package for arranging plots and other graphic elements (text, tables etc) in different grid arrangements. The basic syntax is to use plot1 | plot2 for side by side charts, and plot1 / plot2for top and bottom charts. You can also combine these two functions for a grid of different size columns (e.g. plot3 / (plot1 | plot2)\n\n# Make some simply plots using the mtcars package\np1 &lt;- ggplot(mtcars) +\n  geom_point(aes(mpg, disp)) +\n  ggtitle(\"Plot 1\")\n\np2 &lt;- ggplot(mtcars) +\n  geom_boxplot(aes(gear, disp, group = gear)) +\n  ggtitle(\"Plot 2\")\n\n# Example of side by side charts\nlibrary(patchwork)\np1 + p2\n\n\n\n\n\n\n\n# Add in a table next to the plot\np1 + gridExtra::tableGrob(mtcars[1:10, c(\"mpg\", \"disp\")])",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Charts</span>"
    ]
  },
  {
    "objectID": "03-charts.html#saving-to-powerpoint",
    "href": "03-charts.html#saving-to-powerpoint",
    "title": "Charts",
    "section": "Saving to powerpoint",
    "text": "Saving to powerpoint\nThere’s a bunch of ways to save ggplot graphics - but the way I find most useful is by exporting to pptx in a common ‘charts’ directory.\nIf you want to save as a png you can use the normal ggsave function - however it will not be editable (e.g. able to click and drag to rescale for a presentation).\nTherefore instead we can use the grattantheme package to easily save to an editable pptx graphic.\nNote: The code below has been commented out so that is will upload to bookdown.org without an error.\n\n# The classic save function to png\n\n#       ggsave(plot = ggplot2::last_plot(),\n#       width = 8,\n#       height = 12,\n#       dpi = 600,\n#       filename = \"/Users/charlescoverdale/Desktop/test.png\")\n\n# Using the grattantheme package to easily safe to powerpoint\n\n#        grattan_save_pptx(p = ggplot2::last_plot(),\n#        \"/Users/charlescoverdale/Desktop/test.pptx\",\n#        type = \"wholecolumn\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Charts</span>"
    ]
  },
  {
    "objectID": "03-charts.html#automating-chart-creation",
    "href": "03-charts.html#automating-chart-creation",
    "title": "Charts",
    "section": "Automating chart creation",
    "text": "Automating chart creation\nLet’s say we have a dataframe of multiple variables. We want to produce simple charts of the same style for each variable (including formatting and titles etc). Sure we can change the aes(x=)and aes(y=) variables in ggplot2 manually for each column - but this is time intensive especially for large data frames. Instead, we can write a function that will loop through the whole dataframe and produce the same format of chart.\n\n# Create a data set\nYear &lt;- c(\"2018\", \"2019\", \"2020\", \"2021\")\n\nVariable1 &lt;- (c(500, 300, 200, 400))\n\nVariable2 &lt;- (c(200, 400, 200, 700))\n\nVariable3 &lt;- (c(300, 500, 800, 1000))\n\n# Combine the columns into a single dataframe\nbar_data_multiple &lt;- (cbind(Year, Variable1, Variable2, Variable3))\nbar_data_multiple &lt;- as.data.frame(bar_data_multiple)\n\n# Change formats to integers\nbar_data_multiple$Variable1 &lt;- as.integer(bar_data_multiple$Variable1)\nbar_data_multiple$Variable2 &lt;- as.integer(bar_data_multiple$Variable2)\nbar_data_multiple$Variable3 &lt;- as.integer(bar_data_multiple$Variable3)\n\n# Define a function\nloop &lt;- function(chart_variable) {\n  ggplot(bar_data_multiple, aes(\n    x = Year,\n    y = .data[[chart_variable]],\n    label = .data[[chart_variable]]\n  )) +\n    geom_bar(stat = \"identity\", fill = \"blue\") +\n    geom_text(\n      aes(\n        label = scales::dollar(.data[[chart_variable]])\n      ),\n      size = 5,\n      col = \"white\",\n      fontface = \"bold\",\n      position = position_stack(vjust = 0.5)\n    ) +\n    labs(\n      title = paste(\"Company X: \",\n        chart_variable,\n        \" (\", head(Year, n = 1),\n        \" - \",\n        tail(Year, n = 1),\n        \")\",\n        sep = \"\"\n      ),\n      subtitle = \"Subtitle goes here\",\n      caption = \"Data: Made up from scratch\",\n      x = \"\",\n      y = \"\"\n    ) +\n    theme_minimal() +\n    theme(plot.title = element_text(face = \"bold\", size = 12)) +\n    theme(plot.subtitle = element_text(size = 11)) +\n    theme(plot.caption = element_text(size = 12)) +\n    theme(axis.text = element_text(size = 12)) +\n    theme(panel.grid.minor = element_blank()) +\n    theme(panel.grid.major.x = element_blank()) +\n    theme(panel.grid.major.y = element_blank()) +\n    theme(\n      axis.title.y = element_blank(),\n      axis.text.y = element_blank(),\n      axis.ticks.y = element_blank()\n    )\n}\n\nplots &lt;- purrr::map(colnames(bar_data_multiple)[colnames(bar_data_multiple) != \"Year\"], loop)\nplots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# cowplot::plot_grid(plotlist = plots)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Charts</span>"
    ]
  },
  {
    "objectID": "04-basic-modelling.html",
    "href": "04-basic-modelling.html",
    "title": "Basic modelling",
    "section": "",
    "text": "Source, format, and plot data\nCreating a model is an essential part of forecasting and data analysis.\nI’ve put together a quick guide on my process for modelling data and checking model fit.\nThe source data I use in this example is Melbourne’s weather record over a 12 month period. Daily temperature is based on macroscale weather and climate systems, however many observable measurements are correlated (i.e. hot days tend to have lots of sunshine). This makes using weather data great for model building.\nBefore we get started, it is useful to have some packages up and running.\n# Useful packages for regression\nlibrary(readr)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(modelr)\nlibrary(cowplot)\nI’ve put together a csv file of weather observations in Melbourne in 2019. We begin our model by downloading the data from Github.\n# Input data\nlink &lt;- \"data/MEL_weather_2019.csv\"\n\n# We'll read this data in as a dataframe\n# The 'check.names' function set to false means the funny units that the BOM use for column names won't affect the import.\n\nMEL_weather_2019 &lt;- read.csv(link, check.names = F)\n\nhead(MEL_weather_2019)\nThis data is relatively clean. One handy change to make is to make the date into a dynamic format (to easily switch between months, years, etc).\n# Add a proper date column\nMEL_weather_2019 &lt;- MEL_weather_2019 %&gt;%\n  mutate(Date = make_date(Year, Month, Day))\nWe also notice that some of the column names have symbols in them. This can be tricky to work with, so let’s rename some columns into something more manageable.\n# Rename key df variables\nnames(MEL_weather_2019)[4] &lt;- \"Solar_exposure\"\nnames(MEL_weather_2019)[5] &lt;- \"Rainfall\"\nnames(MEL_weather_2019)[6] &lt;- \"Max_temp\"\n\nhead(MEL_weather_2019)\nWe’re aiming to investigate if other weather variables can predict maximum temperatures. Solar exposure seems like a plausible place to start. We start by plotting the two variables to if there is a trend.\n# Plot the data\nMEL_temp_investigate &lt;- ggplot(MEL_weather_2019) +\n  geom_point(aes(y = Max_temp, x = Solar_exposure), col = \"grey\") +\n  labs(\n    title = \"Does solar exposure drive temperature in Melbourne?\",\n    caption = \"Data: Bureau of Meteorology 2020\"\n  ) +\n  xlab(\"Solar exposure\") +\n  ylab(\"Maximum temperature °C\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  theme_bw() +\n  theme(axis.text = element_text(size = 10)) +\n  theme(panel.grid.minor = element_blank())\n\nMEL_temp_investigate\nEyeballing the chart above, there seems to be a correlation between the two data sets. We’ll do one more quick plot to analyse the data. What is the distribution of temperature?\nggplot(MEL_weather_2019, aes(x = Max_temp)) +\n  geom_histogram(aes(y = ..density..), colour = \"black\", fill = \"lightblue\") +\n  geom_density(alpha = .5, fill = \"grey\", colour = \"darkblue\") +\n  scale_x_continuous(\n    breaks = c(5, 10, 15, 20, 25, 30, 35, 40, 45),\n    expand = c(0, 0)\n  ) +\n  xlab(\"Temperature\") +\n  ylab(\"Density\") +\n  theme_bw() +\n  theme(axis.text = element_text(size = 12)) +\n  theme(panel.grid.minor = element_blank())\nWe can see here the data is right skewed (i.e. the mean will be greater than the median). We’ll need to keep this in mind. Let’s start building a model.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basic modelling</span>"
    ]
  },
  {
    "objectID": "04-basic-modelling.html#build-a-linear-model",
    "href": "04-basic-modelling.html#build-a-linear-model",
    "title": "Basic modelling",
    "section": "Build a linear model",
    "text": "Build a linear model\nWe start by looking whether a simple linear regression of solar exposure seems to be correlated with temperature. In R, we can use the linear model (lm) function.\n\n# Create a straight line estimate to fit the data\ntemp_model &lt;- lm(Max_temp ~ Solar_exposure, data = MEL_weather_2019)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basic modelling</span>"
    ]
  },
  {
    "objectID": "04-basic-modelling.html#analyse-the-model-fit",
    "href": "04-basic-modelling.html#analyse-the-model-fit",
    "title": "Basic modelling",
    "section": "Analyse the model fit",
    "text": "Analyse the model fit\nLet’s see how well solar exposure explains changes in temperature\n\n# Call a summary of the model\nsummary(temp_model)\n\nThe adjusted R squared value (one measure of model fit) is 0.3596. Furthermore the coefficient of our solar_exposure variable is statistically significant.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basic modelling</span>"
    ]
  },
  {
    "objectID": "04-basic-modelling.html#compare-the-predicted-values-with-the-actual-values",
    "href": "04-basic-modelling.html#compare-the-predicted-values-with-the-actual-values",
    "title": "Basic modelling",
    "section": "Compare the predicted values with the actual values",
    "text": "Compare the predicted values with the actual values\nWe can use this lm function to predict values of temperature based on the level of solar exposure. We can then compare this to the actual temperature record, and see how well the model fits the data set.\n\n# Use this lm model to predict the values\nMEL_weather_2019 &lt;- MEL_weather_2019 %&gt;%\n  mutate(predicted_temp = predict(temp_model, newdata = MEL_weather_2019))\n\n# Calculate the prediction interval\nprediction_interval &lt;- predict(temp_model,\n  newdata = MEL_weather_2019,\n  interval = \"prediction\"\n)\nsummary(prediction_interval)\n\n# Bind this prediction interval data back to the main set\nMEL_weather_2019 &lt;- cbind(MEL_weather_2019, prediction_interval)\nMEL_weather_2019\n\nModel fit is easier to interpret graphically. Let’s plot the data with the model overlaid.\n\n# Plot a chart with data and model on it\nMEL_temp_predicted &lt;-\n  ggplot(MEL_weather_2019) +\n  geom_point(aes(y = Max_temp, x = Solar_exposure),\n    col = \"grey\"\n  ) +\n  geom_line(aes(y = predicted_temp, x = Solar_exposure),\n    col = \"blue\"\n  ) +\n  geom_smooth(aes(y = Max_temp, x = Solar_exposure),\n    method = lm\n  ) +\n  geom_line(aes(y = lwr, x = Solar_exposure),\n    colour = \"red\", linetype = \"dashed\"\n  ) +\n  geom_line(aes(y = upr, x = Solar_exposure),\n    colour = \"red\", linetype = \"dashed\"\n  ) +\n  labs(\n    title =\n      \"Does solar exposure drive temperature in Melbourne?\",\n    subtitle = \"Investigation using linear regression\",\n    caption = \"Data: Bureau of Meteorology 2020\"\n  ) +\n  xlab(\"Solar exposure\") +\n  ylab(\"Maximum temperature °C\") +\n  scale_x_continuous(\n    expand = c(0, 0),\n    breaks = c(0, 5, 10, 15, 20, 25, 30, 35, 40)\n  ) +\n  theme_bw() +\n  theme(axis.text = element_text(size = 10)) +\n  theme(panel.grid.minor = element_blank())\n\nMEL_temp_predicted\n\n\n\n\n\n\n\n\nThis chart includes the model (blue line), confidence interval (grey band around the blue line), and a prediction interval (red dotted line). A prediction interval reflects the uncertainty around a single value (put simple: what is the reasonable upper and lower bound that this data point could be estimated at?). A confidence interval reflects the uncertainty around the mean prediction values (put simply: what is a reasonable upper and lower bound for the blue line at this x value?). Therefore, a prediction interval will be generally much wider than a confidence interval for the same value.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basic modelling</span>"
    ]
  },
  {
    "objectID": "04-basic-modelling.html#analyse-the-residuals",
    "href": "04-basic-modelling.html#analyse-the-residuals",
    "title": "Basic modelling",
    "section": "Analyse the residuals",
    "text": "Analyse the residuals\n\n# Add the residuals to the series\nresiduals_temp_predict &lt;- MEL_weather_2019 %&gt;%\n  add_residuals(temp_model)\n\nPlot these residuals in a chart.\n\nresiduals_temp_predict_chart &lt;-\n  ggplot(\n    data = residuals_temp_predict,\n    aes(x = Solar_exposure, y = resid), col = \"grey\"\n  ) +\n  geom_ref_line(h = 0, colour = \"blue\", size = 1) +\n  geom_point(col = \"grey\") +\n  xlab(\"Solar exposure\") +\n  ylab(\"Maximum temperature (°C)\") +\n  theme_bw() +\n  labs(title = \"Residual values from the linear model\") +\n  theme(axis.text = element_text(size = 12)) +\n  scale_x_continuous(expand = c(0, 0))\nresiduals_temp_predict_chart",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basic modelling</span>"
    ]
  },
  {
    "objectID": "04-basic-modelling.html#linear-regression-with-more-than-one-variable",
    "href": "04-basic-modelling.html#linear-regression-with-more-than-one-variable",
    "title": "Basic modelling",
    "section": "Linear regression with more than one variable",
    "text": "Linear regression with more than one variable\nThe linear model above is *okay*, but can we make it better? Let’s start by adding in some more variables into the linear regression.\nRainfall data might assist our model in predicting temperature. Let’s add in that variable and analyse the results.\n\ntemp_model_2 &lt;-\n  lm(Max_temp ~ Solar_exposure + Rainfall, data = MEL_weather_2019)\nsummary(temp_model_2)\n\nWe can see that adding in rainfall made the model better (R squared value has increased to 0.4338).\nNext, we consider whether solar exposure and rainfall might be related to each other, as well as to temperature. For our third temperature model, we add an interaction variable between solar exposure and rainfall.\n\ntemp_model_3 &lt;- lm(\n  Max_temp ~ Solar_exposure +\n    Rainfall +\n    Solar_exposure:Rainfall,\n  data = MEL_weather_2019\n)\nsummary(temp_model_3)\n\nWe now see this variable is significant, and improves the model slightly (seen by an adjusted R squared of 0.4529).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basic modelling</span>"
    ]
  },
  {
    "objectID": "04-basic-modelling.html#fitting-a-polynomial-regression",
    "href": "04-basic-modelling.html#fitting-a-polynomial-regression",
    "title": "Basic modelling",
    "section": "Fitting a polynomial regression",
    "text": "Fitting a polynomial regression\nWhen analysing the above data set, we see the issue is the sheer variance of temperatures associated with every other variable (it turns out weather forecasting is notoriously difficult).\nHowever we can expect that temperature follows a non-linear pattern throughout the year (in Australia it is hot in January-March, cold in June-August, then starts to warm up again). A linear model (e.g. a straight line) will be a very bad model for temperature — we need to introduce polynomials.\nFor simplicity, we will introduce a new variable (Day_number) which is the day of the year (e.g. 1 January is #1, 31 December is #366).\n\nMEL_weather_2019 &lt;- MEL_weather_2019 %&gt;%\n  mutate(Day_number = row_number())\nhead(MEL_weather_2019)\n\nUsing the same dataset as above, let’s plot temperature in Melbourne in 2019.\n\nMEL_temp_chart &lt;-\n  ggplot(MEL_weather_2019) +\n  geom_line(aes(x = Day_number, y = Max_temp)) +\n  labs(\n    title = \"Melbourne temperature profile\",\n    subtitle = \"Daily maximum temperature recorded in Melbourne in 2019\",\n    caption = \"Data: Bureau of Meteorology 2020\"\n  ) +\n  xlab(\"Day of the year\") +\n  ylab(\"Temperature\") +\n  theme_bw()\n\nMEL_temp_chart\n\n\n\n\n\n\n\n\nWe can see we’ll need a non-linear model to fit this data.\nBelow we create a few different models. We start with a normal straight line model, then add an x² and x³ model. We then use these models and the ‘predict’ function to see what temperatures they forecast based on the input data.\n\n# Create a straight line estimate to fit the data\npoly1 &lt;- lm(Max_temp ~ poly(Day_number, 1, raw = TRUE),\n  data = MEL_weather_2019\n)\nsummary(poly1)\n# Create a polynominal of order 2 to fit this data\npoly2 &lt;- lm(Max_temp ~ poly(Day_number, 2, raw = TRUE),\n  data = MEL_weather_2019\n)\nsummary(poly2)\n# Create a polynominal of order 3 to fit this data\npoly3 &lt;- lm(Max_temp ~ poly(Day_number, 3, raw = TRUE),\n  data = MEL_weather_2019\n)\nsummary(poly3)\n\n# Use these models to predict\nMEL_weather_2019 &lt;- MEL_weather_2019 %&gt;%\n  mutate(poly1values = predict(poly1, newdata = MEL_weather_2019)) %&gt;%\n  mutate(poly2values = predict(poly2, newdata = MEL_weather_2019)) %&gt;%\n  mutate(poly3values = predict(poly3, newdata = MEL_weather_2019))\n\nhead(MEL_weather_2019)\n\nIn the table above we can see the estimates for that data point from the various models.\nTo see how well the models did graphically, we can plot the original data series with the polynominal models overlaid.\n\n# Plot a chart with all models on it\nMEL_weather_model_chart &lt;-\n  ggplot(MEL_weather_2019) +\n  geom_line(aes(x = Day_number, y = Max_temp), col = \"grey\") +\n  geom_line(aes(x = Day_number, y = poly1values), col = \"red\") +\n  geom_line(aes(x = Day_number, y = poly2values), col = \"green\") +\n  geom_line(aes(x = Day_number, y = poly3values), col = \"blue\") +\n\n  # Add text annotations\n  geom_text(x = 10, y = 18, label = \"data series\", col = \"grey\", hjust = 0) +\n  geom_text(x = 10, y = 16, label = \"linear\", col = \"red\", hjust = 0) +\n  geom_text(x = 10, y = 13, label = parse(text = \"x^2\"), col = \"green\", hjust = 0) +\n  geom_text(x = 10, y = 10, label = parse(text = \"x^3\"), col = \"blue\", hjust = 0) +\n  labs(\n    title = \"Estimating Melbourne's temperature\",\n    subtitle = \"Daily maximum temperature recorded in Melbourne in 2019\",\n    caption = \"Data: Bureau of Meteorology 2020\"\n  ) +\n  xlim(0, 366) +\n  ylim(10, 45) +\n  scale_x_continuous(\n    breaks =\n      c(15, 45, 75, 105, 135, 165, 195, 225, 255, 285, 315, 345),\n    labels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"),\n    expand = c(0, 0),\n    limits = c(0, 366)\n  ) +\n  scale_y_continuous(breaks = c(10, 15, 20, 25, 30, 35, 40, 45)) +\n  xlab(\"\") +\n  ylab(\"°C\") +\n  theme_bw() +\n  theme(axis.text = element_text(size = 12)) +\n  theme(panel.grid.minor = element_blank())\n\nMEL_weather_model_chart\n\n\n\n\n\n\n\n\nWe can see in the chart above the polynomial models do much better at fitting the data. However, they are still highly variant.\nJust how variant are they? We can look at the residuals to find out. The residuals is the gap between the observed data point (i.e. the grey line) and our model.\n\n# Get the residuals for poly1\nresiduals_poly1 &lt;- MEL_weather_2019 %&gt;%\n  add_residuals(poly1)\nresiduals_poly1_chart &lt;-\n  ggplot(data = residuals_poly1, aes(x = Day_number, y = resid)) +\n  geom_ref_line(h = 0, colour = \"red\", size = 1) +\n  geom_line() +\n  xlab(\"\") +\n  ylab(\"°C\") +\n  theme_bw() +\n  theme(axis.text = element_text(size = 12)) +\n  theme(\n    axis.ticks.x = element_blank(),\n    axis.text.x = element_blank()\n  )\nresiduals_poly1_chart\n\n\n\n\n\n\n\n# Get the residuals for poly2\nresiduals_poly2 &lt;- MEL_weather_2019 %&gt;%\n  add_residuals(poly2)\nresiduals_poly2_chart &lt;- ggplot(data = residuals_poly2, aes(x = Day_number, y = resid)) +\n  geom_ref_line(h = 0, colour = \"green\", size = 1) +\n  geom_line() +\n  xlab(\"\") +\n  ylab(\"°C\") +\n  theme_bw() +\n  theme(axis.text = element_text(size = 12)) +\n  theme(\n    axis.ticks.x = element_blank(),\n    axis.text.x = element_blank()\n  )\n\nresiduals_poly2_chart\n\n\n\n\n\n\n\n# Get the residuals for poly3\nresiduals_poly3 &lt;- MEL_weather_2019 %&gt;%\n  add_residuals(poly3)\nresiduals_poly3_chart &lt;- ggplot(data = residuals_poly3, aes(x = Day_number, y = resid)) +\n  geom_ref_line(h = 0, colour = \"blue\", size = 1) +\n  geom_line() +\n  theme_bw() +\n  theme(axis.text = element_text(size = 12)) +\n  scale_x_continuous(\n    breaks =\n      c(15, 45, 75, 105, 135, 165, 195, 225, 255, 285, 315, 345),\n    labels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"),\n    expand = c(0, 0),\n    limits = c(0, 366)\n  ) +\n  xlab(\"\") +\n  ylab(\"°C\")\n\nresiduals_poly3_chart\n\n\n\n\n\n\n\nthree_charts_single_page &lt;- plot_grid(\n  residuals_poly1_chart,\n  residuals_poly2_chart,\n  residuals_poly3_chart,\n  ncol = 1, nrow = 3, label_size = 16\n)\nthree_charts_single_page\n\n\n\n\n\n\n\n\nAs we move from a linear, to a x², to a x³ model, we see the residuals decrease in volatility.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basic modelling</span>"
    ]
  },
  {
    "objectID": "05-hypothesis-testing.html",
    "href": "05-hypothesis-testing.html",
    "title": "Hypothesis testing",
    "section": "",
    "text": "A quick refresher\nHypothesis testing is a way of validating if a claim about a population (e.g. a data set) is correct. Getting data on a whole population (e.g. everyone in Australia) is hard. So, to validate a hypothesis, we use random samples from a population instead.\nThe language when dealing with hypothesis testing is purposefully janky.\nWhen looking at the outputs of our hypothesis test, we consider p-values. Note: There’s lots wrong with p-values that we won’t bother getting into right now. The long story short is if you make your null hypothesis ultra specific and only report when your p-value on your millionth iteration of a test is below 0.05… bad science is likely to get published and cited.\nWhat we need to know:\nLet’s load in some packages and get started.\n# Load necessary packages\nlibrary(ggridges)\nlibrary(ggplot2)\nlibrary(ggrepel) # Avoid overlapping text in plots\nlibrary(viridis) # Color scales\nlibrary(readxl) # Read Excel files\nlibrary(dplyr) # Data manipulation\nlibrary(stringr) # String operations\nlibrary(tidyr) # Data tidying (replaces reshape)\nlibrary(lubridate) # Work with dates\nlibrary(gapminder) # Gapminder dataset\nlibrary(ggalt) # Extensions to ggplot (dumbbell plots, etc.)\nlibrary(purrr) # Functional programming\nlibrary(scales) # Scaling tools for ggplot\nlibrary(aTSA) # Time series analysis\nlibrary(readrba) # For working with Reserve Bank of Australia data",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "05-hypothesis-testing.html#a-quick-refresher",
    "href": "05-hypothesis-testing.html#a-quick-refresher",
    "title": "Hypothesis testing",
    "section": "",
    "text": "A small p-value (typically less than or equal to 0.05) indicates strong evidence against the null hypothesis, so we reject it.\nA large p-value (greater than 0.05) indicates weak evidence against the null hypothesis, so you fail to reject it.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "05-hypothesis-testing.html#t-testing-our-first-hypothesis",
    "href": "05-hypothesis-testing.html#t-testing-our-first-hypothesis",
    "title": "Hypothesis testing",
    "section": "T-testing our first hypothesis",
    "text": "T-testing our first hypothesis\nWe’ll start by creating a normally distributed random dataset using rnorm.\nBy default the rnorm function will generate a dataset that has a mean of 0 and a standard deviation of 1, but let’s state it explicitly to keep things simple.\n\nset.seed(40)\ndataset1 &lt;- data.frame(variable1 = rnorm(1000, mean = 0, sd = 1))\n\nLet’s chart the distribution.\n\nggplot() +\n  geom_histogram(aes(x = dataset1$variable1, y = ..density..),\n    binwidth = 0.1, fill = \"blue\", alpha = 0.5\n  ) +\n  stat_function(fun = dnorm, args = list(\n    mean = mean(dataset1$variable1),\n    sd = sd(dataset1$variable1)\n  )) +\n  geom_vline(xintercept = 0, linetype = \"dotted\", alpha = 0.5) +\n  labs(title = \"Histogram for T-Testing\", x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 12)) +\n  theme(plot.subtitle = element_text(size = 11)) +\n  theme(plot.caption = element_text(size = 8)) +\n  theme(axis.text = element_text(size = 9)) +\n  theme(panel.grid.minor = element_blank()) +\n  theme(panel.grid.major.x = element_blank()) +\n  theme(plot.subtitle = element_text(margin = ggplot2::margin(0, 0, 15, 0))) +\n  theme(plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), \"cm\"))\n\n\n\n\n\n\n\n\nWe know that the mean of dataset1 will be approximately zero (because we set it)… but let’s check anyway.\n\n# Find the mean of dataset1\nmean(dataset1$variable1)\n\nNow let’s run our first hypothesis test. We’ll use the t.test function. This is in the format of t.test(data, null_hypothesis).\nWe’ll start with the null hypothesis that the mean for dataset1$variable1 is 5. This is a two tailed t-test, as we will reject the null if we’re confident the mean is either above or below 5.\n\n# Hypothesis test\nt.test(dataset1$variable1, mu = 5)\n\nWe see the p-value here is tiny, meaning we reject the null hypothesis. That is to say, the mean for dataset1$variable1 is not 5.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "05-hypothesis-testing.html#understanding-tailed-tests",
    "href": "05-hypothesis-testing.html#understanding-tailed-tests",
    "title": "Hypothesis testing",
    "section": "Understanding tailed tests",
    "text": "Understanding tailed tests\nBy default, t.test assumes a two-tailed test with a 95% confidence level. However, sometimes we need to test if one variable is greater or smaller than another (rather than just different from the null). This is when we use one-tailed tests.\nNext, we test whether the mean is -0.03 using a one-tailed test:\n\n# Hypothesis test\nt.test(dataset1$variable1, mu = -0.03, alternative = \"greater\")\n\nWe see here the p-value is greater than 0.05, leading us to fail to reject the null hypothesis. In a sentence, we cannot say that the mean of dataset1$variable1 is different to 0.01.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "05-hypothesis-testing.html#correlation",
    "href": "05-hypothesis-testing.html#correlation",
    "title": "Hypothesis testing",
    "section": "Correlation",
    "text": "Correlation\nA correlation coefficient measures the direction and strength of the relationship between two variables. However, correlation calculations assume normal distributions.\nLet’s examine the relationship between mpg and hp in the mtcars dataset.\n\nggplot2::ggplot(mtcars, aes(x = hp, y = mpg)) +\n  geom_point(color = \"blue\", alpha = 0.5) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Horsepower vs. Miles Per Gallon\",\n    x = \"Horsepower\",\n    y = \"Miles Per Gallon\"\n  ) +\n  theme_minimal(base_size = 8) +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(face = \"bold\", size = 12),\n    plot.subtitle = element_text(size = 11, margin = ggplot2::margin(0, 0, 25, 0)),\n    plot.caption = element_text(size = 8),\n    axis.text = element_text(size = 8),\n    axis.title.y = element_text(margin = ggplot2::margin(r = 3)),\n    axis.text.y = element_text(vjust = -0.5, margin = ggplot2::margin(l = 20, r = -10)),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    axis.line.x = element_line(colour = \"black\", size = 0.4),\n    axis.ticks.x = element_line(colour = \"black\", size = 0.4)\n  )\n\n\n\n\n\n\n\n\nWe see that miles per gallon is correlated with horsepower. It’s a negative relationship, meaning the more horsepower in a car, the less miles per gallon the car exhibits.\nWe can use the cor.test to tell us the correlation coefficient and the p-value of the correlation.\nWe specify the method as ‘pearson’ for the Pearson correlation coefficient.\n\ncor.test(mtcars$hp, mtcars$mpg, method = \"pearson\")\n\nSince Pearson’s method assumes normality, we check the distributions. Let’s plot a histogram for both hp and mpg.\n\nggplot() +\n  geom_histogram(aes(x = mtcars$mpg, y = ..density..), fill = \"blue\", alpha = 0.5) +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = mean(mtcars$mpg), sd = sd(mtcars$mpg))\n  ) +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = mean(mtcars$mpg), linetype = \"dotted\", alpha = 0.5) +\n  labs(\n    title = \"Histogram of mtcars$mpg\",\n    caption = \"Data: Made from mtcars\",\n    x = \"\",\n    y = \"\"\n  ) +\n  theme_minimal() +\n  theme(panel.spacing.x = unit(10, \"mm\")) +\n  theme(legend.position = \"none\") +\n  theme(plot.title = element_text(face = \"bold\", size = 12)) +\n  theme(plot.subtitle = element_text(size = 11)) +\n  theme(plot.caption = element_text(size = 8)) +\n  theme(axis.text = element_text(size = 9)) +\n  theme(panel.grid.minor = element_blank()) +\n  theme(panel.grid.major.x = element_blank()) +\n  theme(plot.title = element_text(margin = ggplot2::margin(0, 0, 15, 0))) +\n  theme(plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), \"cm\"))\n\n\n\n\n\n\n\n\n\nggplot() +\n  geom_histogram(aes(x = mtcars$hp, y = ..density..), fill = \"blue\", alpha = 0.5) +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = mean(mtcars$hp), sd = sd(mtcars$hp))\n  ) +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = mean(mtcars$hp), linetype = \"dotted\", alpha = 0.5) +\n  labs(\n    title = \"Histogram of mtcars$hp\",\n    caption = \"Data: Made from mtcars\",\n    x = \"\",\n    y = \"\"\n  ) +\n  theme_minimal() +\n  theme(panel.spacing.x = unit(10, \"mm\")) +\n  theme(legend.position = \"none\") +\n  theme(plot.title = element_text(face = \"bold\", size = 12)) +\n  theme(plot.subtitle = element_text(size = 11)) +\n  theme(plot.caption = element_text(size = 8)) +\n  theme(axis.text = element_text(size = 9)) +\n  theme(panel.grid.minor = element_blank()) +\n  theme(panel.grid.major.x = element_blank()) +\n  theme(plot.title = element_text(margin = ggplot2::margin(0, 0, 15, 0))) +\n  theme(plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), \"cm\"))\n\n\n\n\n\n\n\n\nCrikey… they don’t look very normal at all.\nLet’s plot QQ plots of our variables and see what’s going on.\n\nggplot(mtcars, aes(sample = mpg)) +\n  geom_qq() +\n  geom_qq_line() +\n  labs(\n    title = \"QQ plot of mtcars$mpg\",\n    caption = \"Data: Made from mtcars\",\n    x = \"\",\n    y = \"\"\n  ) +\n  theme_minimal() +\n  theme(panel.spacing.x = unit(10, \"mm\")) +\n  theme(legend.position = \"none\") +\n  theme(plot.title = element_text(face = \"bold\", size = 12)) +\n  theme(plot.subtitle = element_text(size = 11)) +\n  theme(plot.caption = element_text(size = 8)) +\n  theme(axis.text = element_text(size = 9)) +\n  theme(panel.grid.minor = element_blank()) +\n  theme(panel.grid.major.x = element_blank()) +\n  theme(plot.title = element_text(margin = ggplot2::margin(0, 0, 15, 0))) +\n  theme(plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), \"cm\"))\n\n\n\n\n\n\n\nggplot(mtcars, aes(sample = hp)) +\n  geom_qq() +\n  geom_qq_line() +\n  labs(\n    title = \"QQ plot of mtcars$hp\",\n    caption = \"Data: Made from mtcars\",\n    x = \"\",\n    y = \"\"\n  ) +\n  theme_minimal() +\n  theme(panel.spacing.x = unit(10, \"mm\")) +\n  theme(legend.position = \"none\") +\n  theme(plot.title = element_text(face = \"bold\", size = 12)) +\n  theme(plot.subtitle = element_text(size = 11)) +\n  theme(plot.caption = element_text(size = 8)) +\n  theme(axis.text = element_text(size = 9)) +\n  theme(panel.grid.minor = element_blank()) +\n  theme(panel.grid.major.x = element_blank()) +\n  theme(plot.title = element_text(margin = ggplot2::margin(0, 0, 15, 0))) +\n  theme(plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), \"cm\"))\n\n\n\n\n\n\n\n\nHmm okay, both series are a bit all over the shop. Let’s do a statistical test to confirm.\nThe Shapiro-Wilk’s method is widely used for normality testing. The null hypothesis of this tests is that the sample distribution is normal. If the test is significant, the distribution is non-normal.\n\nshapiro.test(mtcars$mpg)\nshapiro.test(mtcars$hp)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "05-hypothesis-testing.html#confidence-intervals-for-the-mean",
    "href": "05-hypothesis-testing.html#confidence-intervals-for-the-mean",
    "title": "Hypothesis testing",
    "section": "Confidence intervals for the mean",
    "text": "Confidence intervals for the mean\nWe can calkcualte the confidence interval for the mean. This measures the variance of the normal distribution, and gives us an idea of how ‘clustered’ the values are to the mean.\nThere are 4 steps to do this:\n\nCalculate the mean\nCalculate the standard error of the mean\nFind the t-score that corresponds to the confidence level\nCalculate the margin of error and construct the confidence interval\n\n\nmpg.mean &lt;- mean(mtcars$mpg)\nprint(mpg.mean)\n\nmpg.n &lt;- length(mtcars$mpg)\nmpg.sd &lt;- sd(mtcars$mpg)\nmpg.se &lt;- mpg.sd / sqrt(mpg.n)\nprint(mpg.se)\n\nalpha &lt;- 0.05\ndegrees.freedom &lt;- mpg.n - 1\nt.score &lt;- qt(p = alpha / 2, df = degrees.freedom, lower.tail = F)\nprint(t.score)\n\nmpg.error &lt;- t.score * mpg.se\n\nlower.bound &lt;- mpg.mean - mpg.error\nupper.bound &lt;- mpg.mean + mpg.error\nprint(c(lower.bound, upper.bound))\n\nFor the lazy folks among us - there’s also this quick and dirty way of doing it.\n\n# Calculate the mean and standard error\nmpg.model &lt;- lm(mpg ~ 1, mtcars)\n\n# Calculate the confidence interval\nconfint(mpg.model, level = 0.95)\n\nGreat. Let’s plot this interval on the distribution.\n\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(aes(y = ..density..), binwidth = 2, fill = \"blue\", alpha = 0.5) +\n  stat_function(fun = dnorm, args = list(mean = mean(mtcars$mpg), sd = sd(mtcars$mpg))) +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = mean(mtcars$mpg), linetype = \"dotted\", alpha = 0.5) +\n  geom_vline(xintercept = c(lower.bound, upper.bound), col = \"purple\") +\n  labs(\n    title = \"Histogram of mtcars$mpg\",\n    caption = \"Data: Made from mtcars\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    panel.spacing.x = unit(10, \"mm\"),\n    legend.position = \"none\",\n    plot.title = element_text(face = \"bold\", size = 12, margin = ggplot2::margin(0, 0, 15, 0)),\n    plot.subtitle = element_text(size = 11),\n    plot.caption = element_text(size = 8),\n    axis.text = element_text(size = 9),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), \"cm\")\n  )\n\n\n\n\n\n\n\n\nTwo things we note here: Firstly, the distribution doesn’t look that normal. Secondly, the 95% confidence interval looks narrow as a result.\nLet’s do the same analysis with an actual normal distribution and see what happens.\n\nset.seed(404)\ndataset2 &lt;- data.frame(variable1 = rnorm(1000, mean = 0, sd = 1))\n\ndf2.mean &lt;- mean(dataset2$variable1)\ndf2.n &lt;- length(dataset2$variable1)\ndf2.sd &lt;- sd(dataset2$variable1)\ndf2.se &lt;- df2.sd / sqrt(df2.n) # Fixed variable name typo\n\nalpha &lt;- 0.05\nt.score &lt;- qt(p = alpha / 2, df = df2.n - 1, lower.tail = FALSE)\n\ndf2.error &lt;- t.score * df2.se\nlower.bound.df2 &lt;- df2.mean - df2.error\nupper.bound.df2 &lt;- df2.mean + df2.error\n\n# Functions to shade the tails\nshade_tail &lt;- function(x, bound, direction) {\n  y &lt;- dnorm(x, mean = 0, sd = 1)\n  y[(direction == \"upper\" & x &lt; bound) | (direction == \"lower\" & x &gt; bound)] &lt;- NA\n  return(y)\n}\n\n# Plot\nggplot(dataset2, aes(x = variable1)) +\n  geom_histogram(aes(y = ..density..), binwidth = 0.1, fill = \"blue\", alpha = 0.5) +\n  stat_function(fun = dnorm, args = list(mean = df2.mean, sd = df2.sd)) +\n  geom_hline(yintercept = 0) +\n  geom_vline(\n    xintercept = c(df2.mean, lower.bound.df2, upper.bound.df2),\n    linetype = c(\"dotted\", \"solid\", \"solid\"), col = c(\"black\", \"purple\", \"purple\"), alpha = 0.5\n  ) +\n  stat_function(\n    fun = function(x) shade_tail(x, upper.bound.df2, \"upper\"),\n    geom = \"area\", fill = \"grey\", col = \"grey\", alpha = 0.8\n  ) +\n  stat_function(\n    fun = function(x) shade_tail(x, lower.bound.df2, \"lower\"),\n    geom = \"area\", fill = \"grey\", col = \"grey\", alpha = 0.8\n  ) +\n  labs(\n    title = \"95% Confidence Interval\",\n    caption = \"Data: Generated from rnorm(1000)\",\n    x = \"\", y = \"\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(-3, 3, by = 1)) +\n  theme(\n    panel.spacing.x = unit(10, \"mm\"),\n    legend.position = \"none\",\n    plot.title = element_text(face = \"bold\", size = 12, margin = ggplot2::margin(0, 0, 15, 0)),\n    plot.subtitle = element_text(size = 11),\n    plot.caption = element_text(size = 8),\n    axis.text = element_text(size = 9),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), \"cm\")\n  )\n\n\n\n\n\n\n\n\nGreat - we’ve got a more sensible looking plot, and greyed out the tails where our confidence interval excludes. We expect out observation to fall somewhere between the two purple lines (or more exactly between -2.1 and 2.1)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "05-hypothesis-testing.html#confidence-intervals-for-a-model",
    "href": "05-hypothesis-testing.html#confidence-intervals-for-a-model",
    "title": "Hypothesis testing",
    "section": "Confidence intervals for a model",
    "text": "Confidence intervals for a model\nWe can also calculate the confidence interval around a linear model. This process shows how confident we can be about any single point in the linear estimate. If the confidence interval is wide, the estimate at that point is likely unreliable.\nWe’ll create a linear model of mpg on the y-axis and horsepower on the x-axis.\n\nmtcars.lm &lt;- lm(mpg ~ hp, data = mtcars)\nsummary(mtcars.lm)\n\npredict(mtcars.lm, newdata = mtcars, interval = \"confidence\")\n\nThegeom_smooth()function presents an easy way to plot a confidence interval on a chart.\nThe syntax in this example is:\ngeom_smooth(aes(x = hp, y = mpg), method='lm', level=0.95)\n\nggplot(mtcars, aes(x = hp, y = mpg)) +\n  geom_point(col = \"blue\", alpha = 0.5) +\n  geom_smooth(method = \"lm\", level = 0.95) +\n  labs(\n    title = \"Building a Regression Model\",\n    subtitle = \"Higher horsepower cars get fewer miles per gallon\",\n    caption = \"Data: mtcars dataset\",\n    x = \"Horsepower\",\n    y = \"Miles per Gallon\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(face = \"bold\", size = 12),\n    plot.subtitle = element_text(size = 11, margin = ggplot2::margin(0, 0, 25, 0)),\n    plot.caption = element_text(size = 8),\n    axis.text = element_text(size = 8),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    axis.title.y = element_text(margin = ggplot2::margin(r = 3)),\n    axis.text.y = element_text(vjust = -0.5, margin = ggplot2::margin(l = 20, r = -10)),\n    axis.line.x = element_line(colour = \"black\", size = 0.4),\n    axis.ticks.x = element_line(colour = \"black\", size = 0.4)\n  )\n\n\n\n\n\n\n\n\nFor a sanity check, let’s crank up the confidence level to 0.999 (meaning our interval should capture just about all the observations). We see the confidence interval band increases… but not by that much. Why?\nWell remember how the data isn’t a very good normal distribution? That means the confidence interval function won’t be super accurate - especially at the extremes.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "06-forecasting.html",
    "href": "06-forecasting.html",
    "title": "Forecasting",
    "section": "",
    "text": "ARIMA\nSo, we’ve got a time series dataset… but what is a reasonable forecast for how it might behave in the future?\nSure we can build a confidence interval (as we learned in the previous chapter) and figure out a reasonable value - but what about forecasting for multiple periods into the future?\nThat’s where we need to build models. Let’s load in some packages.\nWe’ll start with some pre-loaded time series data. The ggplot2 package includes a data set called ‘economics’ that contains US economic indicators from the 1960’s to 2015.\nAs a side note: We can also get Australian unemployment rate data using the readrba function.\nLet’s plot the US data to see what we are working with.\nAutoRegressive Integrated Moving Average (ARIMA) models are a handy tool to have in the toolbox. An ARIMA model describes where Yt depends on its own lags. A moving average (MA only) model is one where Yt depends only on the lagged forecast errors. We combine these together (technically we integrate them) and get ARIMA.\nWhen working with ARIMAs, we need to ‘difference’ our series to make it stationary.\nWe check if it is stationary using the augmented Dickey-Fuller test. The null hypothesis assumes that the series is non-stationary. A series is said to be stationary when its mean, variance, and autocovariance don’t change much over time.\n# Test for stationarity\naTSA::adf.test(econ_data$uempmed)\n\n# See the auto correlation\nacf(econ_data$uempmed)\n\n\n\n\n\n\n\n# Identify partial auto correlation\npacf(econ_data$uempmed)\n\n\n\n\n\n\n\n# Take the first differences of the series\necon_data &lt;- econ_data %&gt;% mutate(diff = uempmed - lag(uempmed))\n\n# Plot the first differences\nggplot(econ_data, aes(x = date, y = diff)) +\n  geom_point(col = \"grey\", alpha = 0.5) +\n  geom_smooth(col = \"blue\") +\n  labs(\n    title = \"1st Difference (Unemployment Rate)\",\n    caption = \"Data: ggplot2::economics\",\n    x = \"\",\n    y = \"\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(face = \"bold\", size = 12, margin = ggplot2::margin(0, 0, 25, 0)),\n    plot.subtitle = element_text(size = 11),\n    plot.caption = element_text(size = 8),\n    axis.text = element_text(size = 8),\n    axis.title.y = element_text(margin = ggplot2::margin(t = 0, r = 3, b = 0, l = 0)),\n    axis.text.y = element_text(vjust = -0.5, margin = ggplot2::margin(l = 20, r = -10)),\n    axis.line.x = element_line(colour = \"black\", size = 0.4),\n    axis.ticks.x = element_line(colour = \"black\", size = 0.4),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank()\n  )\n# Fit an ARIMA model\nARIMA_model &lt;- forecast::auto.arima(econ_data$uempmed)\n\n# Display model summary and residual diagnostics\nsummary(ARIMA_model)\nforecast::checkresiduals(ARIMA_model)\n\n\n\n\n\n\n\n# Forecast for the next 36 time periods\nARIMA_forecast &lt;- forecast::forecast(ARIMA_model, h = 36, level = c(95))\n\n# Plot the forecast\nplot(ARIMA_forecast)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "06-forecasting.html#machine-learning",
    "href": "06-forecasting.html#machine-learning",
    "title": "Forecasting",
    "section": "Machine learning",
    "text": "Machine learning\nThere probably isn’t a hotter term this decade than ‘machine learning’. But the principles of getting machines to perform operations based on unknown inputs has been around for the best part of 100 years. ‘Figure it out’ is now an instruction you can give a computer — and with careful programming and enough data, we can get pretty close.\nThe modern standard for machine learning in R is tidymodels — a collection of packages that gives you a clean, consistent way to build, train, and evaluate models. It’s replaced the older caret package and is now the go-to approach for most R users doing ML work.\n\nlibrary(tidymodels)\nlibrary(ranger) # Fast random forest engine",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "06-forecasting.html#linear-regression-with-tidymodels",
    "href": "06-forecasting.html#linear-regression-with-tidymodels",
    "title": "Forecasting",
    "section": "Linear regression with tidymodels",
    "text": "Linear regression with tidymodels\nWe know that a model is just any function (of one or more variables) that helps to explain observations. Let’s start with the simplest case: a linear regression predicting miles per gallon from horsepower using the mtcars dataset.\nThe nice thing about tidymodels is that every model follows the same pattern: define the model spec, bundle it into a workflow, fit it, then evaluate. Once you’ve learned it once, swapping between model types is trivial.\n\ndata(mtcars)\n\n# Split into training (80%) and testing (20%) sets\nset.seed(123)\nmtcars_split &lt;- initial_split(mtcars, prop = 0.8)\nmtcars_train &lt;- training(mtcars_split)\nmtcars_test &lt;- testing(mtcars_split)\n\n# Define a linear regression model\nlm_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\n# Bundle model + formula into a workflow\nlm_workflow &lt;- workflow() %&gt;%\n  add_model(lm_spec) %&gt;%\n  add_formula(mpg ~ hp)\n\n# Fit on training data\nlm_fit &lt;- lm_workflow %&gt;% fit(data = mtcars_train)\n\nWe can visualise how well the model fits by plotting predicted vs actual values.\n\n# Generate predictions on test set\nlm_preds &lt;- predict(lm_fit, mtcars_test) %&gt;% bind_cols(mtcars_test)\n\nggplot(lm_preds, aes(x = hp)) +\n  geom_point(aes(y = mpg), colour = \"blue\", alpha = 0.6) +\n  geom_line(aes(y = .pred), colour = \"blue\") +\n  geom_segment(aes(y = mpg, yend = .pred, xend = hp),\n    colour = \"black\", alpha = 0.5, linetype = \"dotted\"\n  ) +\n  labs(\n    title = \"Linear Regression: Horsepower vs Fuel Efficiency\",\n    subtitle = \"Higher horsepower cars get fewer miles per gallon\",\n    caption = \"Data: mtcars\",\n    x = \"Horsepower\",\n    y = \"Miles per Gallon\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 12),\n    plot.subtitle = element_text(size = 11, margin = ggplot2::margin(b = 25)),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\nTo put a number on how well the model performed, yardstick (part of tidymodels) gives us RMSE, MAE, and R² in a single call — no manual calculations needed.\n\nmetrics(lm_preds, truth = mpg, estimate = .pred)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "06-forecasting.html#random-forest",
    "href": "06-forecasting.html#random-forest",
    "title": "Forecasting",
    "section": "Random forest",
    "text": "Random forest\nTrue machine learning takes this further by building many models across many parameter combinations to find the best predictors. Random forests are one of the most reliable approaches — they combine hundreds of decision trees and average their predictions, reducing overfitting.\nWe’ll use the Boston housing dataset to predict median home values (medv) from neighbourhood characteristics.\n\n# Load Boston dataset\ndata(\"Boston\", package = \"MASS\")\n\n# Split into training (80%) and testing (20%)\nset.seed(123)\nboston_split &lt;- initial_split(Boston, prop = 0.8)\nboston_train &lt;- training(boston_split)\nboston_test &lt;- testing(boston_split)\n\nWe define a random forest model using rand_forest() with the ranger engine — a fast, modern implementation.\n\n# Define random forest model\nrf_spec &lt;- rand_forest(trees = 500) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\n# Bundle into workflow\nrf_workflow &lt;- workflow() %&gt;%\n  add_model(rf_spec) %&gt;%\n  add_formula(medv ~ .)\n\n# Fit on training data\nrf_fit &lt;- rf_workflow %&gt;% fit(data = boston_train)\n\nNow evaluate performance on the held-out test set.\n\n# Generate predictions\nrf_preds &lt;- predict(rf_fit, boston_test) %&gt;% bind_cols(boston_test)\n\n# Evaluate\nmetrics(rf_preds, truth = medv, estimate = .pred)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "06-forecasting.html#comparing-models",
    "href": "06-forecasting.html#comparing-models",
    "title": "Forecasting",
    "section": "Comparing models",
    "text": "Comparing models\nOne of the best things about tidymodels is how easy it is to swap models in and out. Here we run a plain linear regression on the same Boston data and compare RMSE directly — same workflow, different model spec.\n\n# Linear regression on Boston data\nlm_boston_spec &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\nlm_boston_fit &lt;- workflow() %&gt;%\n  add_model(lm_boston_spec) %&gt;%\n  add_formula(medv ~ .) %&gt;%\n  fit(data = boston_train)\n\nlm_boston_preds &lt;- predict(lm_boston_fit, boston_test) %&gt;% bind_cols(boston_test)\n\n# Compare RMSE\nrf_rmse &lt;- rmse(rf_preds, truth = medv, estimate = .pred)$.estimate\nlm_rmse &lt;- rmse(lm_boston_preds, truth = medv, estimate = .pred)$.estimate\n\ncat(\"Random forest RMSE:\", round(rf_rmse, 2), \"\\n\")\ncat(\"Linear regression RMSE:\", round(lm_rmse, 2), \"\\n\")\n\nThe random forest will typically outperform linear regression on this dataset — it captures non-linear relationships between neighbourhood characteristics and home values that a straight line cannot.\nAs we see, our randomForest model performed much better, with a RMSE of 2.96 compared to 4.58.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "07-web-scraping.html",
    "href": "07-web-scraping.html",
    "title": "Web scraping",
    "section": "",
    "text": "Why it matters\nCollecting data off websites can be a nightmare. The worst case is manually typing data from a web-page into spreadsheets… but there are many steps we can do before resorting to that.\nThis chapter will outline the process for pulling data off the web, and particularly for understanding the exact web-page element we want to extract.\nThe notes and code loosely follow the fabulous data tutorial by Grant R. McDermott in his Data Science for Economists series. It has been updated to scrape the most recent version and structure of the relevant Wikipedia pages.\nFirst up, let’s load some packages.\n# Install development version of rvest if necessary\nif (numeric_version(packageVersion(\"rvest\")) &lt; numeric_version(\"0.99.0\")) {\n  remotes::install_github(\"tidyverse/rvest\")\n}\n\n# Load and install the packages that we'll be using today\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse, rvest, lubridate, janitor, data.table, hrbrthemes)\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyverse)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Web scraping</span>"
    ]
  },
  {
    "objectID": "07-web-scraping.html#anatomy-of-a-webpage",
    "href": "07-web-scraping.html#anatomy-of-a-webpage",
    "title": "Web scraping",
    "section": "Anatomy of a webpage",
    "text": "Anatomy of a webpage\nWeb pages can be categorized as either server-side rendered (where content is embedded in the HTML) or client-side rendered (where content loads dynamically using JavaScript). When scraping server-side rendered pages, locating the correct CSS or XPath selectors is crucial.\nTrawling through CSS code on a webpage is a bit of a nightmare - so we’ll use a chrome extension called SelectGadget to help.\nThe R package that’s going to do the heavy lifting is called rvest and is based on the python package called Beauty Soup.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Web scraping</span>"
    ]
  },
  {
    "objectID": "07-web-scraping.html#scraping-a-table",
    "href": "07-web-scraping.html#scraping-a-table",
    "title": "Web scraping",
    "section": "Scraping a table",
    "text": "Scraping a table\nLet’s use this wikipedia page as a starting example. It contains various entries for the men’s 100m running record.\nWe can start by pulling all the data from the webpage.\n\nm100 &lt;- rvest::read_html(\n  \"http://en.wikipedia.org/wiki/Men%27s_100_metres_world_record_progression\"\n)\nm100\n\n{html_document}\n&lt;html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 skin-theme-clientpref-day vector-sticky-header-enabled wp25eastereggs-enable-clientpref-1 vector-toc-available\" lang=\"en\" dir=\"ltr\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body class=\"skin--responsive skin-vector skin-vector-search-vue mediawik ...\n\n\n…and we get a whole heap of mumbo jumbo.\nTo get the table of ‘Unofficial progression before the IAAF’ we’re going to have to be more specific.\nUsing the SelectGadget tool we can click around and identify that that specific table.\n\npre_iaaf &lt;-\n  m100 %&gt;%\n  html_element(\"div+ .wikitable :nth-child(1)\") %&gt;% ## select table element\n  html_table() ## convert to data frame\n\npre_iaaf\n\n# A tibble: 21 × 5\n    Time Athlete               Nationality    `Location of races`     Date      \n   &lt;dbl&gt; &lt;chr&gt;                 &lt;chr&gt;          &lt;chr&gt;                   &lt;chr&gt;     \n 1  10.8 Luther Cary           United States  Paris, France           4 July 18…\n 2  10.8 Cecil Lee             United Kingdom Brussels, Belgium       25 Septem…\n 3  10.8 Étienne De Ré         Belgium        Brussels, Belgium       4 August …\n 4  10.8 L. Atcherley          United Kingdom Frankfurt/Main, Germany 13 April …\n 5  10.8 Harry Beaton          United Kingdom Rotterdam, Netherlands  28 August…\n 6  10.8 Harald Anderson-Arbin Sweden         Helsingborg, Sweden     9 August …\n 7  10.8 Isaac Westergren      Sweden         Gävle, Sweden           11 Septem…\n 8  10.8 Isaac Westergren      Sweden         Gävle, Sweden           10 Septem…\n 9  10.8 Frank Jarvis          United States  Paris, France           14 July 1…\n10  10.8 Walter Tewksbury      United States  Paris, France           14 July 1…\n# ℹ 11 more rows\n\n\nNiiiiice - now that’s better. Let’s do some quick data cleaning.\n\npre_iaaf &lt;- pre_iaaf %&gt;%\n  clean_names() %&gt;%\n  mutate(date = mdy(date))\n\npre_iaaf\n\n# A tibble: 21 × 5\n    time athlete               nationality    location_of_races       date      \n   &lt;dbl&gt; &lt;chr&gt;                 &lt;chr&gt;          &lt;chr&gt;                   &lt;date&gt;    \n 1  10.8 Luther Cary           United States  Paris, France           NA        \n 2  10.8 Cecil Lee             United Kingdom Brussels, Belgium       NA        \n 3  10.8 Étienne De Ré         Belgium        Brussels, Belgium       NA        \n 4  10.8 L. Atcherley          United Kingdom Frankfurt/Main, Germany NA        \n 5  10.8 Harry Beaton          United Kingdom Rotterdam, Netherlands  NA        \n 6  10.8 Harald Anderson-Arbin Sweden         Helsingborg, Sweden     NA        \n 7  10.8 Isaac Westergren      Sweden         Gävle, Sweden           1998-11-18\n 8  10.8 Isaac Westergren      Sweden         Gävle, Sweden           1999-10-18\n 9  10.8 Frank Jarvis          United States  Paris, France           NA        \n10  10.8 Walter Tewksbury      United States  Paris, France           NA        \n# ℹ 11 more rows\n\n\nLet’s also scrape the data for the more recent running records. That’s the tables named ‘Records (1912-1976)’ and ‘Records since 1977’.\nFor the second table:\n\niaaf_76 &lt;- m100 %&gt;%\n  html_element(\"#mw-content-text &gt; div &gt; table:nth-child(17)\") %&gt;%\n  html_table()\n\niaaf_76 &lt;- iaaf_76 %&gt;%\n  clean_names() %&gt;%\n  mutate(date = mdy(date))\n\niaaf_76\n\n# A tibble: 54 × 8\n    time wind   auto athlete       nationality location_of_race date       ref  \n   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;            &lt;date&gt;     &lt;chr&gt;\n 1  10.6 \"\"     NA   Donald Lippi… United Sta… Stockholm, Swed… NA         [2]  \n 2  10.6 \"\"     NA   Jackson Scho… United Sta… Stockholm, Swed… NA         [2]  \n 3  10.4 \"\"     NA   Charley Padd… United Sta… Redlands, USA    NA         [2]  \n 4  10.4 \"0.0\"  NA   Eddie Tolan   United Sta… Stockholm, Swed… 2029-08-19 [2]  \n 5  10.4 \"\"     NA   Eddie Tolan   United Sta… Copenhagen, Den… NA         [2]  \n 6  10.3 \"\"     NA   Percy Willia… Canada      Toronto, Canada  2030-09-19 [2]  \n 7  10.3 \"0.4\"  10.4 Eddie Tolan   United Sta… Los Angeles, USA 2032-01-19 [2]  \n 8  10.3 \"\"     NA   Ralph Metcal… United Sta… Budapest, Hunga… 2033-12-19 [2]  \n 9  10.3 \"\"     NA   Eulace Peaco… United Sta… Oslo, Norway     2034-06-19 [2]  \n10  10.3 \"\"     NA   Chris Berger  Netherlands Amsterdam, Neth… NA         [2]  \n# ℹ 44 more rows\n\n\nAnd now for the third table:\n\niaaf &lt;- m100 %&gt;%\n  html_element(\"#mw-content-text &gt; div.mw-parser-output &gt; table:nth-child(23)\") %&gt;%\n  html_table() %&gt;%\n  clean_names() %&gt;%\n  mutate(date = mdy(date))\niaaf\n\n# A tibble: 24 × 9\n    time wind   auto athlete        nationality   location_of_race    date  \n   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;         &lt;chr&gt;               &lt;date&gt;\n 1 10.1  1.3      NA Bob Hayes      United States Tokyo, Japan        NA    \n 2 10.0  0.8      NA Jim Hines      United States Sacramento, USA     NA    \n 3 10.0  2.0      NA Charles Greene United States Mexico City, Mexico NA    \n 4  9.95 0.3      NA Jim Hines      United States Mexico City, Mexico NA    \n 5  9.93 1.4      NA Calvin Smith   United States Colorado Springs, … NA    \n 6  9.83 1.0      NA Ben Johnson    Canada        Rome, Italy         NA    \n 7  9.93 1.0      NA Carl Lewis     United States Rome, Italy         NA    \n 8  9.93 1.1      NA Carl Lewis     United States Zürich, Switzerland NA    \n 9  9.79 1.1      NA Ben Johnson    Canada        Seoul, South Korea  NA    \n10  9.92 1.1      NA Carl Lewis     United States Seoul, South Korea  NA    \n# ℹ 14 more rows\n# ℹ 2 more variables: notes_note_2 &lt;chr&gt;, duration_of_record &lt;chr&gt;\n\n\nHow good. Now let’s bind the rows together to make a master data set.\n\nwr100 &lt;- rbind(\n  pre_iaaf %&gt;% dplyr::select(time, athlete, nationality, date) %&gt;%\n    mutate(era = \"Pre-IAAF\"),\n  iaaf_76 %&gt;% dplyr::select(time, athlete, nationality, date) %&gt;%\n    mutate(era = \"Pre-automatic\"),\n  iaaf %&gt;% dplyr::select(time, athlete, nationality, date) %&gt;%\n    mutate(era = \"Modern\")\n)\n\nwr100\n\n# A tibble: 99 × 5\n    time athlete               nationality    date       era     \n   &lt;dbl&gt; &lt;chr&gt;                 &lt;chr&gt;          &lt;date&gt;     &lt;chr&gt;   \n 1  10.8 Luther Cary           United States  NA         Pre-IAAF\n 2  10.8 Cecil Lee             United Kingdom NA         Pre-IAAF\n 3  10.8 Étienne De Ré         Belgium        NA         Pre-IAAF\n 4  10.8 L. Atcherley          United Kingdom NA         Pre-IAAF\n 5  10.8 Harry Beaton          United Kingdom NA         Pre-IAAF\n 6  10.8 Harald Anderson-Arbin Sweden         NA         Pre-IAAF\n 7  10.8 Isaac Westergren      Sweden         1998-11-18 Pre-IAAF\n 8  10.8 Isaac Westergren      Sweden         1999-10-18 Pre-IAAF\n 9  10.8 Frank Jarvis          United States  NA         Pre-IAAF\n10  10.8 Walter Tewksbury      United States  NA         Pre-IAAF\n# ℹ 89 more rows\n\n\nExcellent. Let’s plot the results.\n\nggplot(wr100) +\n  geom_point(aes(x = date, y = time, col = era), alpha = 0.7) +\n  labs(\n    title = \"Men's 100m World Record Progression\",\n    subtitle = \"Analysing how times have improved over the past 130 years\",\n    caption = \"Data: Wikipedia 2025\",\n    x = \"\",\n    y = \"\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(limits = c(9.5, 11), breaks = c(9.5, 10, 10.5, 11)) +\n  theme(\n    axis.text.y = element_text(vjust = -0.5, margin = ggplot2::margin(l = 20, r = -20)),\n    plot.subtitle = element_text(margin = ggplot2::margin(0, 0, 25, 0), size = 11),\n    legend.title = element_blank(),\n    plot.title = element_text(face = \"bold\", size = 12),\n    plot.caption = element_text(size = 8),\n    axis.text = element_text(size = 8),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    axis.line.x = element_line(colour = \"black\", size = 0.4),\n    axis.ticks.x = element_line(colour = \"black\", size = 0.4)\n  )",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Web scraping</span>"
    ]
  },
  {
    "objectID": "08-text-mining.html",
    "href": "08-text-mining.html",
    "title": "Text mining",
    "section": "",
    "text": "Frequency analysis\nNumbers are great… but words literally tell a story. Analysing text (e.g. books, tweets, survey responses) in a quantitative format is naturally challenging - however there’s a few tricks which can simplify the process.\nThis chapter outlines the process for inputting text data, and running some simple analysis. The notes and code loosely follow the fabulous book Text Mining with R by Julia Silge and David Robinson.\nFirst up, let’s load some packages.\nThere’s a online depository called Project Gutenberg which catalogue texts that have lost their copyright.\nIt just so happens that The Bible is on this list. Let’s check out the most frequent words.\nlibrary(tidyverse)\nlibrary(tidytext)\n\n# Correct URL for the raw text file\nbible_url &lt;- \"https://raw.githubusercontent.com/charlescoverdale/casualdabbler2e/main/data/bible.txt\"\n\n# Read the text file directly from the URL\nbible &lt;- read_lines(bible_url)\n\n# Convert to a tibble\nbible_df &lt;- tibble(text = bible)\n\n# Tokenize words and remove stop words\nbible_tidy &lt;- bible_df %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words, by = \"word\")\n\n# Find and display the most common words\ncommon_words &lt;- bible_tidy %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  head(20) # Show the top 20 words\n\nprint(common_words)\n\n# A tibble: 20 × 2\n   word       n\n   &lt;chr&gt;  &lt;int&gt;\n 1 lord    7830\n 2 thou    5474\n 3 thy     4600\n 4 god     4446\n 5 ye      3983\n 6 thee    3827\n 7 1       2830\n 8 2       2724\n 9 3       2570\n10 israel  2565\n11 4       2476\n12 son     2370\n13 7       2351\n14 5       2308\n15 6       2297\n16 hath    2264\n17 king    2264\n18 9       2210\n19 8       2193\n20 people  2142\nSomewhat unsurprisingly - “lord” wins it by a country mile.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text mining</span>"
    ]
  },
  {
    "objectID": "08-text-mining.html#sentiment-analysis",
    "href": "08-text-mining.html#sentiment-analysis",
    "title": "Text mining",
    "section": "Sentiment analysis",
    "text": "Sentiment analysis\nJust like a frequency analysis, we can do a ‘vibe’ analysis (i.e. sentiment of a text) using a clever thesaurus matching technique.\nIn the tidytext package are lexicons which include the general sentiment of words (e.g. the emotion you can use to describe that word).\nLet’s see the count of words most associated with ‘joy’ in the bible.\n\n# Tokenize words\nbible_tidy &lt;- bible_df %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  mutate(word = tolower(word)) # Ensure lowercase matching\n\n# Get NRC lexicon & filter for \"joy\"\nnrcjoy &lt;- tidytext::get_sentiments(\"nrc\") %&gt;%\n  filter(sentiment == \"joy\")\n\n# Join words with NRC joy sentiment list & count occurrences\nbible_joy_words &lt;- bible_tidy %&gt;%\n  inner_join(nrcjoy, by = \"word\") %&gt;%\n  count(word, sort = TRUE)\n\n# View top joyful words\nprint(bible_joy_words)\n\n# A tibble: 264 × 2\n   word         n\n   &lt;chr&gt;    &lt;int&gt;\n 1 god       4446\n 2 good       720\n 3 art        494\n 4 peace      429\n 5 found      404\n 6 glory      402\n 7 daughter   324\n 8 pray       313\n 9 love       310\n10 blessed    302\n# ℹ 254 more rows",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text mining</span>"
    ]
  },
  {
    "objectID": "09-geocoding.html",
    "href": "09-geocoding.html",
    "title": "Geocoding",
    "section": "",
    "text": "Lats and longs\nGeocoding converts addresses into coordinates (latitude and longitude) and vice versa. This helps turn basic address data into spatial points, making it useful for mapping and analyzing location-based patterns.\nFor example, geocoding can help define catchment areas around an asset or visualize spatial data on a map. To do this, we need access to a large database of addresses and coordinates. Google Maps is a popular choice, offering an API with a freemium model for access.\nThis guide by Oleksandr Titorchuk also provides a great overview of the geocoding process in R.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Geocoding</span>"
    ]
  },
  {
    "objectID": "09-geocoding.html#spatial-data",
    "href": "09-geocoding.html#spatial-data",
    "title": "Geocoding",
    "section": "Spatial data",
    "text": "Spatial data\nLet’s get started by loading up some packages.\n\n# Loads the required required packages\npacman::p_load(\n  ggmap,\n  tmaptools,\n  RCurl,\n  jsonlite,\n  tidyverse,\n  leaflet,\n  writexl,\n  readr,\n  readxl,\n  sf,\n  mapview,\n  rgdal\n)\n\nNext, we’ll introduce some address data.\nApproved speed camera locations in Victoria, Australia are publicly available through the government website. We can download the dataset as a spreadsheet and import it into R as a csv.\n\n# Read in spreadsheet\nurl &lt;- \"https://raw.githubusercontent.com/charlescoverdale/bookdata/main/mobile-camera-locations-june-2022.csv\"\n\n# Note: We need to import as a csv rather than xlsx for url functionality\ncamera_address &lt;- read.csv(url, header = TRUE)\n\n# Fix the column labels\ncamera_address &lt;- janitor::row_to_names(camera_address, 1)\n\n# Convert the suburb column to title case\ncamera_address$SUBURB &lt;- str_to_title(camera_address$SUBURB)\n\n# Concatenate the two fields into a single address field\ncamera_address$ADDRESS &lt;- paste(camera_address$LOCATION,\n  camera_address$SUBURB,\n  sep = \", \"\n)\n\n# Add in Australia to the address field just to idiot proof the df\ncamera_address$ADDRESS &lt;- paste(camera_address$ADDRESS, \", Australia\", sep = \"\")\n\n# Preview the data\nhead(camera_address)\n\n          LOCATION            SUBURB Reason Code Audit Date\n2        Abey Road        Cobblebank         BCD     May-22\n3      Adam Street     Golden Square         ACD     Apr-22\n4        Agar Road       Coronet Bay          BC          T\n5    Airport Drive Melbourne Airport        ABCD     Apr-22\n6 Aitken Boulevard     Roxburgh Park         ABC     Apr-22\n7 Aitken Boulevard       Craigieburn        ABCD     Apr-22\n                                      ADDRESS\n2            Abey Road, Cobblebank, Australia\n3       Adam Street, Golden Square, Australia\n4           Agar Road, Coronet Bay, Australia\n5 Airport Drive, Melbourne Airport, Australia\n6  Aitken Boulevard, Roxburgh Park, Australia\n7    Aitken Boulevard, Craigieburn, Australia\n\n\nThis dataset does not have street numbers - only street names. This makes intuitive sense (as most speed cameras aren’t placed outside residential houses, but rather along main roads).\nWe can see that the street and suburb names are in different columns. To run a geocode, we want a single field of address data. This makes it easier for the database we’re fuzzy matching against to select the best possible point. Let’s append the street name and the suburb name into a single field.\nWe know all these addresses are in Australia, so we can add the word ‘Australia’ to the end of each entry in the newly created address field to help the geocoder find the location.\nIt makes sense that all these addressed should have a postcode in the format 3XXX (as the data set is for the State of Victoria). However, let’s hold off on that assumption for now.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Geocoding</span>"
    ]
  },
  {
    "objectID": "09-geocoding.html#google-api",
    "href": "09-geocoding.html#google-api",
    "title": "Geocoding",
    "section": "Google API",
    "text": "Google API\nNow that we have a useful and highly descriptive address field, we’re ready to run the geocode.\nWe can geocode the lats and lons using Google’s API through ther ggmap package.\n\nWe must register a API key (by creating an account in the google developer suite).\nYou can ping the API 2,500 times a day. Lucky for us this dataset is only 1,800 rows long!\n\nUncomment the chunk below (using Ctrl+Shift+C) and enter your unique key from google to run the code.\n\n# #Input the google API key\n# register_google(key = \"PASTE YOUR UNIQUE KEY HERE\")\n#\n# #Run the geocode function from ggmap package\n# camera_ggmap &lt;- ggmap::geocode(location = camera_address$ADDRESS,\n#                          output = \"more\",\n#                          source = \"google\")\n#\n# #We'll bind the newly created address columns to the original df\n# camera_ggmap &lt;- cbind(camera_address, camera_ggmap)\n#\n# #Print the results\n# head(camera_ggmap)\n#\n# #Write the data to a df\n# readr::write_csv(camera_ggmap,\"C:/Data/camera_geocoded.csv\")\n\nWe’ll load up the output this chunk generates and continue.\n\nurl &lt;- \"https://raw.githubusercontent.com/charlescoverdale/bookdata/main/camera_geocoded.csv\"\n\n# Note: We need to import as a csv rather than xlsx for url functionality\ncamera_ggmap &lt;- read.csv(url, header = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Geocoding</span>"
    ]
  },
  {
    "objectID": "09-geocoding.html#geocode-analysis",
    "href": "09-geocoding.html#geocode-analysis",
    "title": "Geocoding",
    "section": "Geocode analysis",
    "text": "Geocode analysis\nThe raw dataset our geocode produced looks good! Although… it could probably use some cleaning.\nLet’s rename this dataset so we don’t loose the original df (and therefore have to run the google query again). This is simply a best practice step to build in some redundancy.\n\ncamera_geocoded &lt;- camera_ggmap\n\n\n# Rename df\ncamera_geocoded_clean &lt;- camera_geocoded\n\n# Rename the API generated address field\ncamera_geocoded_clean &lt;- camera_geocoded_clean %&gt;% rename(address_long = address)\n\n# Select only the columns we need\ncamera_geocoded_clean &lt;- camera_geocoded_clean %&gt;%\n  dplyr::select(\n    \"LOCATION\",\n    \"SUBURB\",\n    \"ADDRESS\",\n    \"lon\",\n    \"lat\",\n    \"address_long\"\n  )\n\n# Make all the column names the same format\ncamera_geocoded_clean &lt;- janitor::clean_names(camera_geocoded_clean)\n\n# We still need to convert address_long to title case\ncamera_geocoded_clean$address_long &lt;- str_to_title(camera_geocoded_clean$address_long)\n\nIf we want to go one step further, we can create spatial points from this list of coordinates. This is a good step for eyeballing the data. We see most of it is in Victoria (as expected!)… but it has picked up a couple of points in Sydney and WA.\nThese are worth investigating separately for correction or exclusion.\nUncomment the mapview function below to see these points on a leaflet map.\n\n# Convert data frame to sf object\ncamera_points &lt;- sf::st_as_sf(\n  x = camera_geocoded_clean,\n  coords = c(\"lon\", \"lat\"),\n  crs = \"+proj=longlat\n                                      +datum=WGS84\n                                      +ellps=WGS84\n                                      +towgs84=0,0,0\"\n)\n\n# Plot an interactive map\n# mapview(camera_points)\n\nWe can export these points as a shapefile using the rgdal package.Let’s now have a look at our edge case datapoints.\nThere’s a couple of ways to do this… but one of the simplest is to extract the postcode as a separate field. We can then simply sort by postcodes that do not start with the number 3.\n\ncamera_points$postcode &lt;- str_sub(camera_points$address_long,\n  start = -16,\n  end = -12\n)\n\ncamera_points$postcode &lt;- as.numeric(camera_points$postcode)\n\noutliers &lt;- camera_points %&gt;% filter(postcode &lt; 3000 | postcode &gt;= 4000)\n\nprint(outliers)\n\nSimple feature collection with 7 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 115.9555 ymin: -33.90941 xmax: 152.7034 ymax: -25.53849\nGeodetic CRS:  +proj=longlat\n                                      +datum=WGS84\n                                      +ellps=WGS84\n                                      +towgs84=0,0,0\n                   location     suburb\n1               Epping Road     Epping\n2               High Street     Epping\n3           Kensington Road Kensington\n4          Maryborough Road      Ascot\n5 Maryborough-Ballarat Road     Talbot\n6  Mooroopna-Murchison Road  Murchison\n7             Plumpton Road   Plumpton\n                                         address\n1                 Epping Road, Epping, Australia\n2                 High Street, Epping, Australia\n3         Kensington Road, Kensington, Australia\n4             Maryborough Road, Ascot, Australia\n5   Maryborough-Ballarat Road, Talbot, Australia\n6 Mooroopna-Murchison Road, Murchison, Australia\n7             Plumpton Road, Plumpton, Australia\n                                   address_long                   geometry\n1         Epping Rd, Epping Nsw 2121, Australia POINT (151.0906 -33.77076)\n2           High St, Epping Nsw 2121, Australia POINT (151.0836 -33.77611)\n3 Kensington Rd, Kensington Nsw 2033, Australia POINT (151.2211 -33.90941)\n4               Maryborough Qld 4650, Australia POINT (152.7034 -25.53849)\n5               Maryborough Qld 4650, Australia POINT (152.7034 -25.53849)\n6                  Murchison Wa 6630, Australia POINT (115.9555 -26.89259)\n7     Plumpton Rd, Plumpton Nsw 2761, Australia POINT (150.8439 -33.74823)\n  postcode\n1     2121\n2     2121\n3     2033\n4     4650\n5     4650\n6     6630\n7     2761\n\n\nEasy enough. We see there’s 7 rows that don’t have a postcode starting in a 3000 postcode.\nFour of these are in NSW, two in QLD, and one in WA. It looks like they are real (e.g. the streets and suburbs do exist in that state)… so for now let’s just exclude them from our dataset. We’ll do this by filtering by postcode.\nUncomment the mapview function below to see these points on a leaflet map.\n\ncamera_points_vic &lt;- camera_points %&gt;% filter(postcode &gt; \"3000\" & postcode &lt; \"4000\")\n\nhead(camera_points_vic)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 144.2738 ymin: -38.43479 xmax: 145.4467 ymax: -36.77596\nGeodetic CRS:  +proj=longlat\n                                      +datum=WGS84\n                                      +ellps=WGS84\n                                      +towgs84=0,0,0\n          location            suburb\n1        Abey Road        Cobblebank\n2      Adam Street     Golden Square\n3        Agar Road       Coronet Bay\n4    Airport Drive Melbourne Airport\n5 Aitken Boulevard     Roxburgh Park\n6 Aitken Boulevard       Craigieburn\n                                      address\n1            Abey Road, Cobblebank, Australia\n2       Adam Street, Golden Square, Australia\n3           Agar Road, Coronet Bay, Australia\n4 Airport Drive, Melbourne Airport, Australia\n5  Aitken Boulevard, Roxburgh Park, Australia\n6    Aitken Boulevard, Craigieburn, Australia\n                                       address_long                   geometry\n1           Abey Rd, Cobblebank Vic 3338, Australia POINT (144.5879 -37.70471)\n2        Adam St, Golden Square Vic 3555, Australia POINT (144.2738 -36.77596)\n3          Agar Rd, Coronet Bay Vic 3984, Australia POINT (145.4467 -38.43479)\n4 Airport Dr, Melbourne Airport Vic 3045, Australia POINT (144.8661 -37.69291)\n5    Aitken Blvd, Roxburgh Park Vic 3064, Australia  POINT (144.915 -37.62426)\n6      Aitken Blvd, Craigieburn Vic 3064, Australia   POINT (144.91 -37.59446)\n  postcode\n1     3338\n2     3555\n3     3984\n4     3045\n5     3064\n6     3064\n\n# Plot an interactive map\n# mapview(camera_points_vic)\n\nThere we go! A clean, geocoded dataset of speed camera locations in Victoria.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Geocoding</span>"
    ]
  },
  {
    "objectID": "10-drivetime-analysis.html",
    "href": "10-drivetime-analysis.html",
    "title": "Drivetime analysis",
    "section": "",
    "text": "OSRM isochrones\nA ‘drive time’ describes how far you can drive (i.e in a car on a public road) in a certain amount of time.\nTo get started, let’s install and load packages.\nUseful links for further reading: Source 1, Source 2\nThe OSRM package (Github) pulls from OpenStreetMap to find travel times based on location.\nThe downside is that the polygons it generates are pretty chunky… i.e. it doesn’t take into account major roads and streets as the key tributaries/arteries of a city area. We can get around this a bit by dialing up the ‘res’ (i.e. the resolution) in the osrmIsochrone function… but it’s only a partial solution.\nUncomment the leaflet function below to view an interactive map.\n# Create a dataframe with the latitude and longitude\nlocations &lt;- tibble::tribble(\n  ~place, ~lon, ~lat,\n  \"Melbourne\", 144.9631, -37.8136\n)\n\n# Run it through the osrm package to calculate isochrones\niso &lt;- osrmIsochrone(\n  loc = c(locations$lon, locations$lat),\n  breaks = seq(from = 0, to = 30, by = 5),\n  res = 50\n)\n\n# Create an interactive map\n# leaflet() %&gt;%\n#   setView(mean(locations$lon), mean(locations$lat), zoom = 7) %&gt;%\n#   addProviderTiles(\"CartoDB.Positron\", group = \"Greyscale\") %&gt;%\n#   addMarkers(lng = locations$lon, lat = locations$lat) %&gt;%\n#   addPolygons(\n#     fill = TRUE, stroke = TRUE, color = \"black\",\n#     weight = 0.5, fillOpacity = 0.2,\n#     data = iso,\n#     group = \"Drive Time\"\n#   )",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Drivetime analysis</span>"
    ]
  },
  {
    "objectID": "11-raster-data.html",
    "href": "11-raster-data.html",
    "title": "Raster data",
    "section": "",
    "text": "Import raster\nRaster data (sometimes referred to as gridded data) is a type of spatial data that is stored in a grid rather than a polygon. Imagine a chessboard of individuals squares covering the Australian landmass compared to 8 different shapes covering each state and territory.\nThe nice thing about gridded data is that all the cells in the grid are the same size - making calculations much easier. The tricky part is those squares are overlayed on a (very roughly) spherical earth. We’ll have to think about mapping projections along the way.\nLet’s get started. First up we need to load some spatial and data crunching packages.\nNext, we want to import annual rainfall data from github (original source available from the Bureau of Meterology)\nrainfall &lt;- raster(\"https://raw.github.com/charlescoverdale/ERF/master/rainan.txt\")\n\nplot(rainfall)\nStraight away we see this is for the whole of Australia (and then some). We’re only interested in what’s going on in QLD… so let’s crop the data down to scale. For this we’ll need to import a shapefile for QLD.\nThe easiest way to do this is using the strayr package, which pulls official ABS boundary files on demand via read_absmap().\n# Import a polygon for the state of Queensland\nQLD_shape &lt;- strayr::read_absmap(\"state2016\") %&gt;%\n  filter(state_name_2016 == \"Queensland\")",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Raster data</span>"
    ]
  },
  {
    "objectID": "11-raster-data.html#choosing-geographies",
    "href": "11-raster-data.html#choosing-geographies",
    "title": "Raster data",
    "section": "Choosing geographies",
    "text": "Choosing geographies\nWe have raster data for the entirety of Australia (and then some as it’s pulled from one of the BOMs satellites). This is a bit messy to work with - so let’s crop the rainfall data from the entire Australian continent to just Queensland.\n\n# Crop data\nr2 &lt;- crop(rainfall, extent(QLD_shape))\nr3 &lt;- mask(r2, QLD_shape)\nplot(r3)\n\n\n\n\n\n\n\n\n]Next up, let’s transform the cropped raster into a data frame that we can use in the ggplot package.\n\nr3_df &lt;- as.data.frame(r3, xy = TRUE)\n\nr3_df &lt;- r3_df %&gt;%\n  filter(rainan != \"NA\")\n\nggplot() +\n  geom_tile(data = r3_df, aes(x = x, y = y, fill = rainan)) +\n  scale_fill_viridis() +\n  coord_equal() +\n  theme(legend.position = \"bottom\") +\n  theme(legend.key.width = unit(1.2, \"cm\")) +\n  labs(\n    title = \"Rainfall in QLD\",\n    subtitle = \"Analysis from the Bureau of Meterology\",\n    caption = \"Data: BOM 2021\",\n    x = \"\",\n    y = \"\",\n    fill = \"(mm)\"\n  ) +\n  theme_minimal() +\n  theme(axis.ticks.x = element_blank(), axis.text.x = element_blank()) +\n  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  # theme(legend.position = \"bottom\")+\n  theme(plot.title = element_text(face = \"bold\", size = 12)) +\n  theme(plot.subtitle = element_text(size = 11)) +\n  theme(plot.caption = element_text(size = 8))\n\n\n\n\n\n\n\n\n\nExcellent, we’ve got a working map of rainfall in Queensland using the ggplot package. We’ll tidy up the map also and add a title and some better colours.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Raster data</span>"
    ]
  },
  {
    "objectID": "11-raster-data.html#analysis-with-raster-data",
    "href": "11-raster-data.html#analysis-with-raster-data",
    "title": "Raster data",
    "section": "Analysis with raster data",
    "text": "Analysis with raster data\nFor our first piece of data analysis, we’re going to look at areas with less than 600mm of annual rainfall. How many of our data points will have less than 600mm of rain? Let’s take a look at the data distribution and find out.\n\nggplot(r3_df) +\n  geom_histogram(aes(rainan), binwidth = 1, col = \"darkblue\") +\n  labs(\n    title = \"Distribution of annual rainfall in QLD\",\n    subtitle = \"Data using a 5x5km grid\",\n    caption = \"Data: Bureau of Meterology 2021\",\n    x = \"Rainfall (mm)\",\n    y = \"\",\n    fill = \"(mm)\"\n  ) +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme(legend.position = \"none\") +\n  theme(plot.title = element_text(face = \"bold\", size = 12)) +\n  theme(plot.subtitle = element_text(size = 11)) +\n  theme(plot.caption = element_text(size = 8))\n\n\n\n\n\n\n\n\nInteresting. The data is heavily right tailed skewed (the mean will be much higher than the median)… and most of the data looks to be between 0-1000mm (this makes sense).\nLet’s create a ‘flag’ column of 0’s and 1’s that shows when a data point is less than 600mm.\n\n# Define flag column and colors\nr3_df &lt;- r3_df %&gt;%\n  mutate(flag_600mm = as.factor(ifelse(rainan &lt;= 600, 1, 0)))\n\nflag_colours &lt;- c(\"grey\", \"#2FB300\")\n\n# Plot\nggplot(r3_df, aes(x = x, y = y, fill = flag_600mm)) +\n  geom_tile() +\n  scale_fill_manual(values = flag_colours) +\n  coord_equal() +\n  theme_minimal() +\n  labs(\n    title = \"Areas with Less than 600mm of Annual Rainfall in QLD\",\n    subtitle = \"Identifying Suitable Land Parcels for ERF Plantings\",\n    caption = \"Data: Bureau of Meteorology 2021\",\n    x = \"\",\n    y = \"\",\n    fill = \"(mm)\"\n  ) +\n  theme(\n    axis.ticks = element_blank(),\n    axis.text = element_blank(),\n    panel.grid = element_blank(),\n    legend.position = \"none\",\n    plot.title = element_text(face = \"bold\", size = 12),\n    plot.subtitle = element_text(size = 11),\n    plot.caption = element_text(size = 8)\n  )",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Raster data</span>"
    ]
  },
  {
    "objectID": "11-raster-data.html#making-an-interactive-map",
    "href": "11-raster-data.html#making-an-interactive-map",
    "title": "Raster data",
    "section": "Making an interactive map",
    "text": "Making an interactive map\nThe static map above is good… but an interactive visual (ideally with place names below) is better. The leaflet package is exceptional here.\n\n# This code creates an interactive leaflet map. It is commented out to allow the html version of RCCD2e to render online.\n\n# leaflet() %&gt;%\n#   addTiles() %&gt;%\n#   addRasterImage(r5, opacity = 0.5) %&gt;%\n#   addLegend(pal = colorNumeric(\"viridis\", values(r5)), values = values(r5))",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Raster data</span>"
    ]
  },
  {
    "objectID": "12-ABS-economic-data.html",
    "href": "12-ABS-economic-data.html",
    "title": "Australian economic data",
    "section": "",
    "text": "GDP\nAustralia has exceptional financial and economic institutions. Three of these institutions release periodic data useful for economic analysis:\nAs usual, there are catches. Most of this data is in inconsistent formats (the reasons for which continue to baffle me). What’s more, it’s currently not possible to ping databases or API’s for access to this data… it is mainly accessed through spreadsheets.\nThe scripts below run through some of the main ways to import, clean, and analyse Australian macroeconomic data in R.\nSome of the key packages we’ll use are readabs and readrba.\nTo get started, let’s install and load packages.\nTo get GDP data from the ABS, we’ll use the read_abs function from the readrba package.\n# For simplicity, we keep the download function seperate to the analysis\nall_gdp &lt;- readabs::read_abs(\"5206.0\", tables = 2)\n# Select the seasonally adjusted data and filter for data and value columns\ngdp_level &lt;- all_gdp %&gt;%\n  filter(\n    series == \"Gross domestic product: Chain volume measures ;\",\n    !is.na(value)\n  ) %&gt;%\n  filter(series_type == \"Seasonally Adjusted\") %&gt;%\n  dplyr::select(date, value) %&gt;%\n  dplyr::rename(quarterly_output = value)\n\ngdp_level &lt;- gdp_level %&gt;%\n  mutate(\n    quarterly_growth_rate =\n      ((quarterly_output / lag(quarterly_output, 1) - 1)) * 100\n  ) %&gt;%\n  mutate(\n    annual_gdp =\n      rollapply(quarterly_output,\n        4,\n        sum,\n        na.rm = TRUE,\n        fill = NA,\n        align = \"right\"\n      )\n  ) %&gt;%\n  mutate(annual_gdp_trillions = annual_gdp / 1000000) %&gt;%\n  mutate(\n    annual_growth_rate =\n      ((annual_gdp / lag(annual_gdp, 4) - 1)) * 100\n  ) %&gt;%\n  mutate(\n    Quarter_of_year =\n      lubridate::quarter(date,\n        with_year = FALSE,\n        fiscal_start = 1\n      )\n  )\n\n# Set a baseline value\ngdp_level$baseline_value &lt;- gdp_level$quarterly_output[\n  which(gdp_level$date == \"2022-03-01\")\n]\n\n\ngdp_level &lt;- gdp_level %&gt;%\n  mutate(\n    baseline_comparison =\n      (quarterly_output / baseline_value) * 100\n  )\n\ntail(gdp_level)\n\n# A tibble: 0 × 9\n# ℹ 9 variables: date &lt;date&gt;, quarterly_output &lt;dbl&gt;,\n#   quarterly_growth_rate &lt;dbl&gt;, annual_gdp &lt;dbl&gt;, annual_gdp_trillions &lt;dbl&gt;,\n#   annual_growth_rate &lt;dbl&gt;, Quarter_of_year &lt;int&gt;, baseline_value &lt;dbl&gt;,\n#   baseline_comparison &lt;dbl&gt;\nNow we can plot the GDP data for Australia.\nplot_gdp &lt;- ggplot(data = gdp_level) +\n  geom_line((aes(x = date, y = annual_gdp_trillions)), col = \"blue\") +\n  labs(\n    title = \"Australian GDP ($AUD)\",\n    subtitle = \"Annualised figures\",\n    caption = \"Data: Australian Bureau of Statistics\",\n    y = \"\",\n    x = \" \"\n  ) +\n  scale_y_continuous(\n    breaks = c(0, 0.5, 1.0, 1.5, 2.0, 2.5),\n    labels = label_number(suffix = \" trillion\")\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  theme(plot.title = element_text(face = \"bold\", size = 12)) +\n  theme(plot.subtitle = element_text(size = 11)) +\n  theme(plot.caption = element_text(size = 8)) +\n  theme(axis.text = element_text(size = 8)) +\n  theme(panel.grid.minor = element_blank()) +\n  theme(panel.grid.major.x = element_blank()) +\n  theme(\n    axis.title.y =\n      element_text(margin = ggplot2::margin(t = 0, r = 0, b = 0, l = 0))\n  ) +\n  theme(axis.text.y = element_text(\n    vjust = -0.5,\n    margin = ggplot2::margin(l = 20, r = -45)\n  )) +\n  theme(axis.line.x = element_line(colour = \"black\", size = 0.4)) +\n  theme(axis.ticks.x = element_line(colour = \"black\", size = 0.4))\n\n\nplot_gdp",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Australian economic data</span>"
    ]
  },
  {
    "objectID": "12-ABS-economic-data.html#unemployment-rate",
    "href": "12-ABS-economic-data.html#unemployment-rate",
    "title": "Australian economic data",
    "section": "Unemployment rate",
    "text": "Unemployment rate\nDownload the data\n\n# Download the time sreies\nall_unemployment &lt;- readabs::read_abs(\"6202.0\", tables = 1)\n\nClean and analyse the data\n\nunemployment_rate &lt;- all_unemployment %&gt;%\n  filter(series == \"Unemployment rate ;  Persons ;\", !is.na(value)) %&gt;%\n  filter(table_title == \"Table 1. Labour force status by Sex, Australia - Trend, Seasonally adjusted and Original\") %&gt;%\n  filter(series_type == \"Seasonally Adjusted\") %&gt;%\n  mutate(mean_unemployment_rate = mean(value)) %&gt;%\n  mutate(percentile_25 = quantile(value, 0.25)) %&gt;%\n  mutate(percentile_75 = quantile(value, 0.75)) %&gt;%\n  dplyr::select(date, value, mean_unemployment_rate, percentile_25, percentile_75)\n\n\ntail(unemployment_rate)\n\n# A tibble: 6 × 5\n  date       value mean_unemployment_rate percentile_25 percentile_75\n  &lt;date&gt;     &lt;dbl&gt;                  &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 2025-08-01  4.25                   6.52          5.23          7.89\n2 2025-09-01  4.45                   6.52          5.23          7.89\n3 2025-10-01  4.32                   6.52          5.23          7.89\n4 2025-11-01  4.30                   6.52          5.23          7.89\n5 2025-12-01  4.09                   6.52          5.23          7.89\n6 2026-01-01  4.08                   6.52          5.23          7.89\n\n\nPlot the data\n\nplot_unemployment_rate &lt;- ggplot(data = unemployment_rate) +\n  geom_line(aes(x = date, y = value), col = \"blue\") +\n  labs(\n    title = \"Unemployment rate\",\n    subtitle = \"Subtitle goes here\",\n    caption = \"Data: Australian Bureau of Statistics\",\n    y = \"Unemployment rate (%)\",\n    x = \" \"\n  ) +\n  scale_y_continuous(labels = scales::comma) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  theme(plot.title = element_text(face = \"bold\", size = 12)) +\n  theme(plot.subtitle = element_text(size = 11)) +\n  theme(plot.caption = element_text(size = 8)) +\n  theme(axis.text = element_text(size = 8)) +\n  theme(panel.grid.minor = element_blank()) +\n  theme(panel.grid.major.x = element_blank()) +\n  theme(\n    axis.title.y =\n      element_text(margin = ggplot2::margin(t = 0, r = 0, b = 0, l = 0))\n  ) +\n  theme(axis.text.y = element_text(\n    vjust = -0.5,\n    margin = ggplot2::margin(l = 20, r = -15)\n  )) +\n  theme(axis.line.x = element_line(colour = \"black\", size = 0.4)) +\n  theme(axis.ticks.x = element_line(colour = \"black\", size = 0.4))\n\nplot_unemployment_rate",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Australian economic data</span>"
    ]
  },
  {
    "objectID": "12-ABS-economic-data.html#inflation-cpi",
    "href": "12-ABS-economic-data.html#inflation-cpi",
    "title": "Australian economic data",
    "section": "Inflation (CPI)",
    "text": "Inflation (CPI)\nDownload the data\n\nall_CPI &lt;- readabs::read_abs(\"6401.0\", tables = c(1, 2))\n\nClean and analyse the data\n\nAustralia_CPI &lt;- all_CPI %&gt;%\n  filter(series == \"Percentage Change from Corresponding Quarter of Previous Year ;  All groups CPI ;  Australia ;\", !is.na(value)) %&gt;%\n  mutate(mean_CPI = mean(value)) %&gt;%\n  mutate(percentile_25 = quantile(value, 0.25)) %&gt;%\n  mutate(percentile_75 = quantile(value, 0.75)) %&gt;%\n  dplyr::select(date, value, mean_CPI, percentile_25, percentile_75)\n\ntail(Australia_CPI)\n\n# A tibble: 0 × 5\n# ℹ 5 variables: date &lt;date&gt;, value &lt;dbl&gt;, mean_CPI &lt;dbl&gt;, percentile_25 &lt;dbl&gt;,\n#   percentile_75 &lt;dbl&gt;\n\n# Can add in the below line to filter\n# filter(date&gt;\"2010-01-01\") %&gt;%\n\nPlot the data\n\nplot_CPI &lt;- ggplot(data = Australia_CPI %&gt;%\n  filter(date &gt; as.Date(\"2000-01-01\"))) +\n  geom_rect(\n    aes(\n      xmin = as.Date(\"2000-01-01\"),\n      xmax = as.Date(\"2025-03-01\"),\n      ymin = 2,\n      ymax = 3\n    ),\n    alpha = 0.1, # Adjusted alpha for better visibility\n    fill = \"lightgrey\"\n  ) +\n  geom_line(aes(x = date, y = value), col = \"blue\") +\n  scale_x_date(date_breaks = \"2 years\", date_labels = \"%Y\") +\n  labs(\n    title = \"Inflation (as measured by the CPI)\",\n    subtitle = \"Subtitle goes here\",\n    caption = \"Data: Australian Bureau of Statistics\",\n    y = \"(%)\",\n    x = \"\"\n  ) +\n  scale_y_continuous(labels = scales::comma) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  theme(plot.title = element_text(face = \"bold\", size = 12)) +\n  theme(plot.subtitle = element_text(size = 11)) +\n  theme(plot.caption = element_text(size = 8)) +\n  theme(axis.text = element_text(size = 8)) +\n  theme(panel.grid.minor = element_blank()) +\n  theme(panel.grid.major.x = element_blank()) +\n  theme(axis.title.y = element_text(margin = ggplot2::margin(t = 0, r = 0, b = 0, l = 0))) +\n  theme(axis.text.y = element_text(\n    vjust = -0.5,\n    margin = ggplot2::margin(l = 20, r = -15)\n  )) +\n  theme(axis.line.x = element_line(colour = \"black\", size = 0.4)) +\n  theme(axis.ticks.x = element_line(colour = \"black\", size = 0.4))\n\nplot_CPI\n\n\n\n\n\n\n\n\nPlot a histogram of the data\n\nplot_CPI_hist &lt;- ggplot(Australia_CPI, aes(x = value)) +\n  geom_histogram(aes(y = ..density..),\n    colour = \"black\", fill = \"lightblue\"\n  ) +\n  geom_density(alpha = .5, fill = \"grey\", colour = \"darkblue\") +\n  scale_x_continuous(expand = c(0, 0)) + # Remove extra space on the x-axis\n\n  labs(\n    title = \"Consumer Price Index: Histogram\",\n    subtitle = \"Subtitle goes here\",\n    caption = \"Data: Australian Bureau of Statistics\",\n    y = \"(%)\",\n    x = \"\"\n  ) +\n  scale_y_continuous(labels = scales::percent, expand = c(0, 0)) + # Ensure no space on y-axis\n\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(face = \"bold\", size = 12),\n    plot.subtitle = element_text(size = 11, margin = ggplot2::margin(b = 15)),\n    plot.caption = element_text(size = 8),\n    axis.text = element_text(size = 8),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    axis.title.y = element_text(margin = ggplot2::margin(t = 0, r = 0, b = 20, l = 0)),\n    axis.text.y = element_text(vjust = -0.5, margin = ggplot2::margin(l = 20, r = -2)),\n    axis.line.x = element_line(colour = \"black\", size = 0.4),\n    axis.ticks.x = element_line(colour = \"black\", size = 0.4)\n  )\n\nplot_CPI_hist",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Australian economic data</span>"
    ]
  },
  {
    "objectID": "12-ABS-economic-data.html#wage-price-index",
    "href": "12-ABS-economic-data.html#wage-price-index",
    "title": "Australian economic data",
    "section": "Wage Price Index",
    "text": "Wage Price Index\nDownload the data\n\nall_wpi &lt;- readabs::read_abs(\"6345.0\")\n\nClean and analyse the data\n\nAustralia_WPI &lt;- all_wpi %&gt;%\n  filter(\n    series == \"Percentage Change From Corresponding Quarter of Previous Year ;  Australia ;  Total hourly rates of pay excluding bonuses ;  Private and Public ;  All industries ;\",\n    !is.na(value)\n  ) %&gt;%\n  filter(series_type == \"Seasonally Adjusted\") %&gt;%\n  mutate(mean_WPI = mean(value)) %&gt;%\n  dplyr::select(date, value, mean_WPI)\n\ntail(Australia_WPI)\n\n# A tibble: 6 × 3\n  date       value mean_WPI\n  &lt;date&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 2024-09-01   3.6     3.13\n2 2024-12-01   3.2     3.13\n3 2025-03-01   3.4     3.13\n4 2025-06-01   3.4     3.13\n5 2025-09-01   3.3     3.13\n6 2025-12-01   3.4     3.13\n\n\nPlot the data\n\nplot_WPI &lt;- ggplot(data = Australia_WPI) +\n  geom_line(aes(x = date, y = value), col = \"blue\") +\n  labs(\n    title = \"Wage Price Index\",\n    subtitle = \"Subtitle goes here\",\n    caption = \"Data: Australian Bureau of Statistics\",\n    y = \"(%)\",\n    x = \" \"\n  ) +\n  scale_y_continuous(labels = scales::comma) +\n  scale_x_date(date_breaks = \"5 years\", date_labels = \"%Y\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  theme(plot.title = element_text(face = \"bold\", size = 12)) +\n  theme(plot.subtitle = element_text(size = 11)) +\n  theme(plot.caption = element_text(size = 8)) +\n  theme(axis.text = element_text(size = 8)) +\n  theme(panel.grid.minor = element_blank()) +\n  theme(panel.grid.major.x = element_blank()) +\n  theme(\n    axis.title.y =\n      element_text(margin = ggplot2::margin(t = 0, r = 0, b = 0, l = 0))\n  ) +\n  theme(axis.text.y = element_text(\n    vjust = -0.5,\n    margin = ggplot2::margin(l = 20, r = -15)\n  )) +\n  theme(axis.line.x = element_line(colour = \"black\", size = 0.4)) +\n  theme(axis.ticks.x = element_line(colour = \"black\", size = 0.4))\n\n\nplot_WPI",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Australian economic data</span>"
    ]
  },
  {
    "objectID": "12-ABS-economic-data.html#aud-exchange-rate",
    "href": "12-ABS-economic-data.html#aud-exchange-rate",
    "title": "Australian economic data",
    "section": "AUD exchange rate",
    "text": "AUD exchange rate\nGod knows why - but there are super quirky names for the official exchange rate tables\nDownload the data\n\nexchange_rate_all &lt;- readrba::read_rba(\n  table_no = (c(\n    \"ex_daily_8386\",\n    \"ex_daily_8790\",\n    \"ex_daily_9194\",\n    \"ex_daily_9598\",\n    \"ex_daily_9902\",\n    \"ex_daily_0306\",\n    \"ex_daily_0709\",\n    \"ex_daily_1013\",\n    \"ex_daily_1417\",\n    \"ex_daily_18cur\"\n  )),\n  cur_hist = \"historical\"\n)\n\nClean and analyse the data\n\nexchange_rate_AUD &lt;- exchange_rate_all %&gt;%\n  filter(series == \"A$1=USD\") %&gt;%\n  dplyr::select(date, value)\n\n\ntail(exchange_rate_AUD)\n\n# A tibble: 6 × 2\n  date       value\n  &lt;date&gt;     &lt;dbl&gt;\n1 2017-12-20 0.766\n2 2017-12-21 0.767\n3 2017-12-22 0.771\n4 2017-12-27 0.774\n5 2017-12-28 0.779\n6 2017-12-29 0.78 \n\n\nPlot the data\n\nplot_exchange_rate_AUD &lt;- ggplot(data = exchange_rate_AUD) +\n  geom_line(aes(x = date, y = value), col = \"blue\") +\n  labs(\n    title = \"AUD Exchange Rate\",\n    subtitle = \"Subtitle goes here\",\n    caption = \"Data: Reserve Bank of Australia\",\n    y = \"Exchange rate\", # Adding a Y-axis label\n    x = \"Date\"\n  ) +\n  scale_y_continuous(labels = scales::comma) +\n  scale_x_date(date_breaks = \"3 years\", date_labels = \"%Y\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 12),\n    plot.subtitle = element_text(size = 11),\n    plot.caption = element_text(size = 8),\n    axis.text = element_text(size = 8),\n    axis.title.y = element_text(margin = ggplot2::margin(t = 0, r = 0, b = 0, l = 0)),\n    axis.text.y = element_text(vjust = -0.5, margin = ggplot2::margin(l = 20, r = -15)),\n    axis.line.x = element_line(colour = \"black\", size = 0.4),\n    axis.ticks.x = element_line(colour = \"black\", size = 0.4),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    legend.position = \"bottom\"\n  )\n\nplot_exchange_rate_AUD",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Australian economic data</span>"
    ]
  },
  {
    "objectID": "13-Austrlaian-election-data.html",
    "href": "13-Austrlaian-election-data.html",
    "title": "Australian election data",
    "section": "",
    "text": "Election maps\nElections tend to create fascinating data sets. They are spatial in nature, comparable over time (i.e. the number of electorates roughly stays the same) - and more importantly they are consequential for all Australians.\nAustralia’s compulsory voting system is a remarkable feature of our Federation. Every three-ish years we all turn out at over 7,000 polling booths our local schools, churches, and community centres to cast a ballot and pick up an obligatory election day sausage. The byproduct is a fascinating longitudinal and spatial data set.\nThe following code explores different R packages, election data sets, and statistical processes aimed at exploring and modelling federal elections in Australia.\nOne word of warning: I use the term electorates, divisions, and seats interchangeably throughout this chapter.\nLet’s start by loading up some packages\nSome phenomenal Australian economists and statisticians have put together a handy election package called eechidna. It includes three main data sets for the most recent Australia federal election (2022).\nThey’ve also gone to the trouble of aggregating some census data to the electorate level. This can be found with the abs2022 function.\nAs noted in the introduction, elections are spatial in nature.\nNot only does geography largely determine policy decisions, we see that many electorates vote for the same party (or even the same candidate) for decades. How electorate boundaries are drawn is a long story (see here, here, and here).\nThe summary version is the AEC carves up the population by state and territory, uses a wacky formula to decide how many seats each state and territory should be allocated, then draws maps to try and get a roughly equal number of people in each electorate.\nOh… and did I mention for reasons that aren’t worth explaining that Tasmania has to have at least 5 seats? Our Federation is a funny thing. Anyhow, at time of writing this is how the breakdown of seats looks.\nNote: The NT doesn’t have the population to justify it’s second seat . The AEC scheduled to dissolve it after the 2019 election but Parliament intervened in late 2020 and a bill was passed to make sure both seats were kept (creating 151 nationally).\nHow variant are these 151 electorates in size? Massive.\nDurack in Western Australia (1.63 million square kilometres) is by far the largest and the smallest is Grayndler in New South Wales (32 square kilometres).\nLet’s make a map to make things easier.\nced2021 &lt;- strayr::read_absmap(\"ced2021\")\n\nCED_map &lt;- ced2021 %&gt;%\n  ggplot() +\n  geom_sf() +\n  labs(\n    title = \"Electoral divisions in Australia\",\n    subtitle = \"It turns out we divide the country in very non-standard blocks\",\n    caption = \"Data: Australian Bureau of Statistics 2016\",\n    x = \"\",\n    y = \"\"\n  ) +\n  theme_minimal() +\n  theme(axis.ticks.x = element_blank(), axis.text.x = element_blank()) +\n  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme(legend.position = \"right\") +\n  theme(plot.title = element_text(face = \"bold\", size = 12)) +\n  theme(plot.subtitle = element_text(size = 11)) +\n  theme(plot.caption = element_text(size = 8))\n\n\nCED_map_remove_6 &lt;- ced2021 %&gt;%\n  dplyr::filter(!ced_code_2021 %in% c(506, 701, 404, 511, 321, 317)) %&gt;%\n  ggplot() +\n  geom_sf() +\n  labs(\n    title = \"194 electoral divisions in Australia\",\n    subtitle = \"Turns out removing the largest 6 electorates makes a difference\",\n    caption = \"Data: Australian Bureau of Statistics 2016\",\n    x = \"\",\n    y = \"\"\n  ) +\n  theme_minimal() +\n  theme(axis.ticks.x = element_blank(), axis.text.x = element_blank()) +\n  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme(legend.position = \"right\") +\n  theme(plot.title = element_text(face = \"bold\", size = 12)) +\n  theme(plot.subtitle = element_text(size = 11)) +\n  theme(plot.caption = element_text(size = 8))\n\nCED_map\nCED_map_remove_6\nNext let’s look at what party/candidate is currently the sitting member for each electorate. To do this on a map we’re going to need to join our tcp22 data and the ced2021 spatial data.\nIn the first data set, the electorate column in called ‘DivisionNm’ and in the second ‘ced_name_2021’.\nWe see the data in our DivisionNm variable is in UPPERCASE while our ced_name_2021 variable is in Titlecase. Let’s change the first variable to Titlecase to match.\nWe can then then join the two dataframes using the left_join function.\n# Pull in the electorate shapefiles via strayr\nelectorates &lt;- strayr::read_absmap(\"ced2021\")\n\n# Make the DivisionNm Titlecase\ntcp22$DivisionNm &lt;- str_to_title(tcp22$DivisionNm)\n\ntcp22_edit &lt;- tcp22 %&gt;%\n  distinct() %&gt;%\n  filter(Elected == \"Y\")\n\n# Make the column names the same\nelectorates &lt;- dplyr::rename(electorates, DivisionNm = ced_name_2021)\n\nced_map_data &lt;- left_join(tcp22_edit, electorates, by = \"DivisionNm\")\n\nced_map_data &lt;- as.data.frame(ced_map_data)\n\nhead(ced_map_data, n = 151)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Australian election data</span>"
    ]
  },
  {
    "objectID": "13-Austrlaian-election-data.html#election-maps",
    "href": "13-Austrlaian-election-data.html#election-maps",
    "title": "Australian election data",
    "section": "",
    "text": "State/Territory\nNumber of members of the House of Representatives (2022)\n\n\n\n\nNew South Wales\n47\n\n\nVictoria\n39\n\n\nQueensland\n30\n\n\nWestern Australia\n15\n\n\nSouth Australia\n10\n\n\nTasmania\n5\n\n\nAustralian Capital Territory\n3\n\n\nNorthern Territory\n2\n\n\nTOTAL\n151",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Australian election data</span>"
    ]
  },
  {
    "objectID": "13-Austrlaian-election-data.html#analysis",
    "href": "13-Austrlaian-election-data.html#analysis",
    "title": "Australian election data",
    "section": "Analysis",
    "text": "Analysis\nLet’s start by answering a simple question: who won the election? For this we’ll need to use the two-candidate preferred data set (to make sure we capture all the minor parties that won seats).\nNote in the table above, the PartyNm variable is a mess. Some candidates noted their party by it’s abbreviation rather than full name, others put in a state specific prefix For this analysis, the PartyAb variable is cleaner to use.\n\nwho_won &lt;- tcp22 %&gt;%\n  filter(Elected == \"Y\") %&gt;%\n  group_by(PartyAb) %&gt;%\n  tally() %&gt;%\n  arrange(desc(n))\n\nhead(who_won, n = 10)\n\n# A tibble: 7 × 2\n  PartyAb     n\n  &lt;chr&gt;   &lt;int&gt;\n1 ALP        77\n2 LP         48\n3 IND        10\n4 NP         10\n5 GRN         4\n6 KAP         1\n7 XEN         1\n\n\nNext up let’s see which candidates won with the smallest percentage of first preference votes. Australia’s preferential voting system normally makes these numbers quite interesting.\n\nwho_won_least_votes_prop &lt;- fp22 %&gt;%\n  filter(Elected == \"Y\") %&gt;%\n  arrange(Percent) %&gt;%\n  mutate(candidate_full_name = paste0(GivenNm, \" \", Surname, \" (\", CandidateID, \")\")) %&gt;%\n  dplyr::select(candidate_full_name, PartyNm, DivisionNm, Percent)\n\nhead(who_won_least_votes_prop, n = 10)\n\n# A tibble: 10 × 4\n   candidate_full_name            PartyNm                DivisionNm   Percent\n   &lt;chr&gt;                          &lt;chr&gt;                  &lt;chr&gt;          &lt;dbl&gt;\n 1 KYLEA JANE TINK (37452)        INDEPENDENT            NORTH SYDNEY    25.2\n 2 SAM BIRRELL (36061)            NATIONAL PARTY         NICHOLLS        26.1\n 3 STEPHEN BATES (37338)          QUEENSLAND GREENS      BRISBANE        27.2\n 4 MICHELLE ANANDA-RAJAH (36433)  AUSTRALIAN LABOR PARTY HIGGINS         28.5\n 5 JUSTINE ELLIOT (36802)         AUSTRALIAN LABOR PARTY RICHMOND        28.8\n 6 BRIAN MITCHELL (37276)         AUSTRALIAN LABOR PARTY LYONS           29.0\n 7 KATE CHANEY (36589)            INDEPENDENT            CURTIN          29.5\n 8 DAI LE (36240)                 INDEPENDENT            FOWLER          29.5\n 9 ELIZABETH WATSON-BROWN (37370) QUEENSLAND GREENS      RYAN            30.2\n10 REBEKHA SHARKIE (37710)        CENTRE ALLIANCE        MAYO            31.4\n\n\nThis is really something.\nThe relationship we’re seeing here seems to be these are many the seats that are won with barely 30% of the first preference vote.\nThe electorate I grew up in is listed here (Richmond) - let’s look at how the votes were allocated.\n\nRichmond_fp &lt;- fp22 %&gt;%\n  filter(DivisionNm == \"RICHMOND\") %&gt;%\n  arrange(-Percent) %&gt;%\n  mutate(candidate_full_name = paste0(GivenNm, \" \", Surname, \" (\", CandidateID, \")\")) %&gt;%\n  dplyr::select(candidate_full_name, PartyNm, DivisionNm, Percent, OrdinaryVotes)\n\nhead(Richmond_fp, n = 10)\n\n# A tibble: 10 × 5\n   candidate_full_name            PartyNm       DivisionNm Percent OrdinaryVotes\n   &lt;chr&gt;                          &lt;chr&gt;         &lt;chr&gt;        &lt;dbl&gt;         &lt;dbl&gt;\n 1 JUSTINE ELLIOT (36802)         AUSTRALIAN L… RICHMOND     28.8          28733\n 2 MANDY NOLAN (36361)            THE GREENS    RICHMOND     25.3          25216\n 3 KIMBERLY HONE (36351)          NATIONAL PAR… RICHMOND     23.4          23299\n 4 GARY BIGGS (36648)             LIBERAL DEMO… RICHMOND      7.7           7681\n 5 TRACEY BELL-HENSELIN (37831)   ONE NATION    RICHMOND      4.08          4073\n 6 ROBERT JAMES MARKS (37084)     UNITED AUSTR… RICHMOND      2.93          2922\n 7 DAVID WARTH (37813)            INDEPENDENT   RICHMOND      2.35          2341\n 8 MONICA SHEPHERD (36408)        INFORMED MED… RICHMOND      2.28          2271\n 9 NATHAN JONES (37721)           INDEPENDENT   RICHMOND      1.98          1974\n10 TERRY PATRICK SHARPLES (37823) INDEPENDENT   RICHMOND      1.28          1274\n\n\nSure enough - the Greens certainly helped get the ALP across the line.\nThe interpretation that these seats are the ‘most marginal’ is incorrect under a preferential voting system (e.g. imagine if ALP win 30% and the Greens win 30% - that is a pretty safe 10% margin assuming traditional preference flows).\nBut - let’s investigate which seats are the most marginal.\n\nwho_won_smallest_margin &lt;- tcp22 %&gt;%\n  filter(Elected == \"Y\") %&gt;%\n  mutate(percent_margin = 2 * (Percent - 50), vote_margin = round(percent_margin * OrdinaryVotes / Percent)) %&gt;%\n  arrange(Percent) %&gt;%\n  mutate(candidate_full_name = paste0(GivenNm, \" \", Surname, \" (\", CandidateID, \")\")) %&gt;%\n  dplyr::select(candidate_full_name, PartyNm, DivisionNm, Percent, OrdinaryVotes, percent_margin, vote_margin)\n\nhead(who_won_smallest_margin, n = 20)\n\n# A tibble: 20 × 7\n   candidate_full_name   PartyNm DivisionNm Percent OrdinaryVotes percent_margin\n   &lt;chr&gt;                 &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n 1 FIONA PHILLIPS (3677… AUSTRA… Gilmore       50.2         56039          0.340\n 2 MICHAEL SUKKAR (3671… LIBERA… Deakin        50.2         50322          0.380\n 3 JAMES STEVENS (37067) LIBERA… Sturt         50.4         56813          0.900\n 4 IAN GOODENOUGH (3659… LIBERA… Moore         50.7         52958          1.32 \n 5 KEITH WOLAHAN (36733) LIBERA… Menzies       50.7         51198          1.36 \n 6 BRIAN MITCHELL (3727… AUSTRA… Lyons         50.9         37341          1.84 \n 7 MARION SCRYMGOUR (37… A.L.P.  Lingiari      51.0         23339          1.90 \n 8 JEROME LAXALE (36827) AUSTRA… Bennelong     51.0         50801          1.96 \n 9 KATE CHANEY (36589)   INDEPE… Curtin        51.3         53847          2.52 \n10 BRIDGET KATHLEEN ARC… LIBERA… Bass          51.4         35288          2.86 \n11 AARON VIOLI (36711)   LIBERA… Casey         51.5         51283          2.96 \n12 DAI LE (36240)        INDEPE… Fowler        51.6         44348          3.26 \n13 PETER DUTTON (37493)  LIBERA… Dickson       51.7         51196          3.40 \n14 MICHELLE ANANDA-RAJA… AUSTRA… Higgins       52.1         49726          4.12 \n15 GORDON REID (36801)   AUSTRA… Robertson     52.3         50277          4.52 \n16 PAT CONAGHAN (36342)  NATION… Cowper        52.3         58204          4.64 \n17 SAM LIM (37337)       AUSTRA… Tangney       52.4         56331          4.76 \n18 SOPHIE SCAMPS (37450) INDEPE… Mackellar     52.5         51973          5    \n19 ELIZABETH WATSON-BRO… QUEENS… Ryan          52.6         52286          5.3  \n20 ALAN TUDGE (36704)    LIBERA… Aston         52.8         51840          5.62 \n# ℹ 1 more variable: vote_margin &lt;dbl&gt;\n\n\nCrikey. We see Fiona Phillips in Gilmore got in with a 0.17% margin (just 380 votes!)\nWhile we’re at it, we better do the opposite and see who romped it in by the largest margin.\n\nwho_won_largest_margin &lt;- tcp22 %&gt;%\n  filter(Elected == \"Y\") %&gt;%\n  mutate(percent_margin = 2 * (Percent - 50), vote_margin = round(percent_margin * OrdinaryVotes / Percent)) %&gt;%\n  arrange(desc(Percent)) %&gt;%\n  mutate(candidate_full_name = paste0(GivenNm, \" \", Surname, \" (\", CandidateID, \")\")) %&gt;%\n  dplyr::select(candidate_full_name, PartyNm, DivisionNm, Percent, OrdinaryVotes, percent_margin, vote_margin)\n\nhead(who_won_largest_margin, n = 20)\n\n# A tibble: 20 × 7\n   candidate_full_name   PartyNm DivisionNm Percent OrdinaryVotes percent_margin\n   &lt;chr&gt;                 &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n 1 DAVID LITTLEPROUD (3… LIBERA… Maranoa       72.1         67153           44.2\n 2 ANDREW WILKIE (33553) INDEPE… Clark         70.8         46668           41.6\n 3 DARREN CHESTER (3604… NATION… Gippsland     70.6         71205           41.1\n 4 ANNE WEBSTER (36060)  NATION… Mallee        69.0         70523           38.0\n 5 SHARON CLAYDON (3680… AUSTRA… Newcastle     68.0         71807           36.0\n 6 MARK COULTON (36363)  NATION… Parkes        67.8         60433           35.7\n 7 ANTHONY ALBANESE (36… AUSTRA… Grayndler     67.0         63413           34.1\n 8 JOSH WILSON (37314)   AUSTRA… Fremantle     66.9         65585           33.8\n 9 MADELEINE KING (3718… AUSTRA… Brand         66.7         63829           33.4\n10 TANYA PLIBERSEK (368… AUSTRA… Sydney        66.7         68770           33.4\n11 TONY PASIN (37083)    LIBERA… Barker        66.6         70483           33.2\n12 DANIEL MULINO (36385) AUSTRA… Fraser        66.5         61251           33  \n13 BARNABY JOYCE (36335) NATION… New Engla…    66.4         64622           32.9\n14 SUSSAN LEY (37032)    LIBERA… Farrer        66.4         66739           32.7\n15 AMANDA RISHWORTH (36… AUSTRA… Kingston      66.4         72564           32.7\n16 ANDREW LEIGH (36234)  AUSTRA… Fenner        65.7         59966           31.4\n17 ANDREW GILES (36447)  AUSTRA… Scullin       65.6         59761           31.2\n18 LINDA BURNEY (36820)  AUSTRA… Barton        65.5         60054           31.1\n19 MATT KEOGH (37195)    AUSTRA… Burt          65.2         59704           30.4\n20 TONY BURKE (36809)    AUSTRA… Watson        65.1         55810           30.2\n# ℹ 1 more variable: vote_margin &lt;dbl&gt;\n\n\nWowza. That’s really something. Some candidates won seats with a 40-45 percent margin - scooping up 70% of the two candidate preferred vote in the process!\nWe can also cut the seats by state for a look at where the ‘strongholds’ are across the country.\n\nwho_won &lt;- tcp22 %&gt;%\n  filter(Elected == \"Y\") %&gt;%\n  group_by(PartyAb, StateAb) %&gt;%\n  tally() %&gt;%\n  arrange(desc(n))\n\nwho_won_by_state &lt;- spread(who_won, StateAb, n) %&gt;% arrange(desc(NSW))\n\nhead(who_won_by_state, n = 10)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Australian election data</span>"
    ]
  },
  {
    "objectID": "13-Austrlaian-election-data.html#trends",
    "href": "13-Austrlaian-election-data.html#trends",
    "title": "Australian election data",
    "section": "Trends",
    "text": "Trends\nNow we’ve figured out how to work with election data - let’s link it up to some AUstralian demographic data. The eechidna package includes a cleaned set of census data from 2022 that has already been adjusted from ASGS boundaries to Commonwealth Electoral Divisions.\n\n# Import the census data from the eechidna package\ndata(eechidna::abs2021)\nhead(abs2021)\n\n# Join with two-party preferred voting data\ndata(tpp10)\nelection2022 &lt;- left_join(abs2021, tpp10, by = \"DivisionNm\")\n\nThat’s what we want to see. 151 rows of data (one for each electorate) and over 80 columns of census variables.\nA starting exploratory exercise is too see which of these variables are correlated with voting for one party or another. There’s some old narrative around LNP voters being rich, old, white, and somehow ‘upper class’ compared to the population at large. Let’s pick a few variables that roughly match with this criteria (Income, Age, English language speakers, and Bachelor educated) and chart it compared to LNP percentage of the vote.\n\n# See relationship between personal income and Liberal/National support\n\nggplot(election2022, aes(x = MedianPersonalIncome, y = LNP_Percent)) +\n  geom_point() +\n  geom_smooth() +\n  theme_minimal()\n\nggplot(election2022, aes(x = MedianAge, y = LNP_Percent)) +\n  geom_jitter() +\n  geom_smooth() +\n  theme_minimal()\n\nggplot(election2022, aes(x = EnglishOnly, y = LNP_Percent)) +\n  geom_jitter() +\n  geom_smooth() +\n  theme_minimal()\n\nggplot(election2022, aes(x = BachelorAbv, y = LNP_Percent)) +\n  geom_jitter() +\n  geom_smooth() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst impressions: Geez this data looks messy.\nSecond impression: Maybe there’s a bit of a trend with age and income?\nLet’s build a regression model to run all the 80 odd census variables in the abs2022 data set against the LNP_percent variable.\n\n# We can use colnames(election2022) to get a big list of all the variables available\n\n# Now we build the model\nelection_model &lt;- lm(\n  LNP_Percent ~\n    Population +\n    Area +\n    Age00_04 +\n    Age05_14 +\n    Age15_19 +\n    Age20_24 +\n    Age25_34 +\n    Age35_44 +\n    Age45_54 +\n    Age55_64 +\n    Age65_74 +\n    Age75_84 +\n    Age85plus +\n    Anglican +\n    AusCitizen +\n    AverageHouseholdSize +\n    BachelorAbv + Born_Asia +\n    Born_MidEast + Born_SE_Europe +\n    Born_UK +\n    BornElsewhere +\n    Buddhism +\n    Catholic +\n    Christianity +\n    Couple_NoChild_House + Couple_WChild_House +\n    CurrentlyStudying + DeFacto +\n    DiffAddress +\n    DipCert +\n    EnglishOnly +\n    FamilyRatio +\n    Finance +\n    HighSchool +\n    Indigenous +\n    InternetUse +\n    Islam +\n    Judaism +\n    Laborer +\n    LFParticipation +\n    Married +\n    MedianAge +\n    MedianFamilyIncome +\n    MedianHouseholdIncome +\n    MedianLoanPay +\n    MedianPersonalIncome +\n    MedianRent +\n    Mortgage +\n    NoReligion +\n    OneParent_House +\n    Owned +\n    Professional +\n    PublicHousing +\n    Renting +\n    SocialServ +\n    SP_House +\n    Tradesperson +\n    Unemployed +\n    Volunteer,\n  data = election2022\n)\n\nsummary(election_model)\n\n\nCall:\nlm(formula = LNP_Percent ~ Population + Area + Age00_04 + Age05_14 + \n    Age15_19 + Age20_24 + Age25_34 + Age35_44 + Age45_54 + Age55_64 + \n    Age65_74 + Age75_84 + Age85plus + Anglican + AusCitizen + \n    AverageHouseholdSize + BachelorAbv + Born_Asia + Born_MidEast + \n    Born_SE_Europe + Born_UK + BornElsewhere + Buddhism + Catholic + \n    Christianity + Couple_NoChild_House + Couple_WChild_House + \n    CurrentlyStudying + DeFacto + DiffAddress + DipCert + EnglishOnly + \n    FamilyRatio + Finance + HighSchool + Indigenous + InternetUse + \n    Islam + Judaism + Laborer + LFParticipation + Married + MedianAge + \n    MedianFamilyIncome + MedianHouseholdIncome + MedianLoanPay + \n    MedianPersonalIncome + MedianRent + Mortgage + NoReligion + \n    OneParent_House + Owned + Professional + PublicHousing + \n    Renting + SocialServ + SP_House + Tradesperson + Unemployed + \n    Volunteer, data = election2022)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.9358  -2.0714  -0.1947   2.1152   8.1823 \n\nCoefficients: (1 not defined because of singularities)\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            2.973e+04  1.408e+04   2.111 0.037912 *  \nPopulation             8.911e-05  4.885e-05   1.824 0.071865 .  \nArea                   7.945e-06  5.069e-06   1.567 0.120957    \nAge00_04              -2.830e+02  1.402e+02  -2.018 0.046910 *  \nAge05_14              -2.923e+02  1.403e+02  -2.084 0.040367 *  \nAge15_19              -2.793e+02  1.401e+02  -1.993 0.049677 *  \nAge20_24              -2.989e+02  1.402e+02  -2.132 0.036042 *  \nAge25_34              -2.921e+02  1.404e+02  -2.080 0.040715 *  \nAge35_44              -2.942e+02  1.401e+02  -2.100 0.038900 *  \nAge45_54              -2.921e+02  1.404e+02  -2.080 0.040689 *  \nAge55_64              -2.903e+02  1.400e+02  -2.073 0.041402 *  \nAge65_74              -2.888e+02  1.401e+02  -2.062 0.042477 *  \nAge75_84              -2.844e+02  1.404e+02  -2.025 0.046152 *  \nAge85plus             -3.014e+02  1.398e+02  -2.156 0.034123 *  \nAnglican               8.452e-01  4.518e-01   1.871 0.065057 .  \nAusCitizen             4.673e-01  7.032e-01   0.664 0.508282    \nAverageHouseholdSize  -2.556e+01  1.684e+01  -1.518 0.133076    \nBachelorAbv           -3.068e+00  8.049e-01  -3.812 0.000270 ***\nBorn_Asia              3.963e-01  3.771e-01   1.051 0.296516    \nBorn_MidEast           1.091e+00  1.029e+00   1.060 0.292396    \nBorn_SE_Europe        -1.628e+00  2.119e+00  -0.768 0.444585    \nBorn_UK                1.487e-01  4.886e-01   0.304 0.761591    \nBornElsewhere          7.110e-01  5.618e-01   1.266 0.209346    \nBuddhism              -1.097e+00  7.070e-01  -1.551 0.124835    \nCatholic              -6.257e-01  4.202e-01  -1.489 0.140356    \nChristianity           5.736e-01  5.095e-01   1.126 0.263605    \nCouple_NoChild_House  -1.581e+00  3.747e+00  -0.422 0.674192    \nCouple_WChild_House   -3.264e-01  4.061e+00  -0.080 0.936150    \nCurrentlyStudying     -4.117e-01  1.241e+00  -0.332 0.740988    \nDeFacto               -6.600e+00  1.931e+00  -3.417 0.000997 ***\nDiffAddress            9.427e-01  3.174e-01   2.970 0.003935 ** \nDipCert               -9.857e-01  6.871e-01  -1.435 0.155319    \nEnglishOnly           -3.977e-01  4.103e-01  -0.969 0.335365    \nFamilyRatio           -2.873e+01  4.547e+01  -0.632 0.529296    \nFinance                1.399e+00  8.717e-01   1.605 0.112372    \nHighSchool             7.160e-01  4.161e-01   1.721 0.089173 .  \nIndigenous             3.919e-01  4.572e-01   0.857 0.393876    \nInternetUse                   NA         NA      NA       NA    \nIslam                 -4.161e-01  5.624e-01  -0.740 0.461601    \nJudaism                4.377e-01  6.511e-01   0.672 0.503385    \nLaborer               -8.258e-01  7.283e-01  -1.134 0.260251    \nLFParticipation        6.822e-01  6.359e-01   1.073 0.286562    \nMarried               -5.852e+00  1.734e+00  -3.375 0.001141 ** \nMedianAge             -9.926e-01  1.036e+00  -0.958 0.340782    \nMedianFamilyIncome    -3.456e-02  2.524e-02  -1.369 0.174711    \nMedianHouseholdIncome  6.410e-02  2.756e-02   2.326 0.022568 *  \nMedianLoanPay         -1.889e-02  1.143e-02  -1.653 0.102290    \nMedianPersonalIncome   2.957e-02  5.171e-02   0.572 0.569043    \nMedianRent             6.781e-03  5.786e-02   0.117 0.906997    \nMortgage               2.021e-02  1.406e+00   0.014 0.988568    \nNoReligion             6.649e-01  4.779e-01   1.391 0.167983    \nOneParent_House       -6.243e+00  3.876e+00  -1.611 0.111150    \nOwned                  1.168e-02  1.334e+00   0.009 0.993036    \nProfessional           1.314e+00  8.840e-01   1.487 0.141064    \nPublicHousing         -8.154e-01  5.754e-01  -1.417 0.160350    \nRenting               -2.602e-02  1.463e+00  -0.018 0.985859    \nSocialServ            -5.254e-02  4.674e-01  -0.112 0.910776    \nSP_House              -2.021e-01  8.735e-01  -0.231 0.817658    \nTradesperson          -1.299e-01  7.929e-01  -0.164 0.870301    \nUnemployed            -4.229e+00  1.503e+00  -2.814 0.006160 ** \nVolunteer              4.773e-01  6.417e-01   0.744 0.459120    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.729 on 80 degrees of freedom\n  (11 observations deleted due to missingness)\nMultiple R-squared:  0.8975,    Adjusted R-squared:  0.822 \nF-statistic: 11.88 on 59 and 80 DF,  p-value: &lt; 2.2e-16\n\n\nFor the people that care about statistical fit and endogenous variables, you may have concerns (and rightly so) with the above approach.\nIt’s pretty rough. Let’s run a basic check to see if the residuals are normally distributed.\n\nhist(election_model$residuals, col = \"bisque\", freq = FALSE, main = NA)\nlines(density(election_model$residuals), col = \"red\")\n\n\n\n\n\n\n\n\nHmm… that’s actually not too bad. Onwards.\nWe see now that only a handful of these variables in the table above are statistically significant. Running an updated (and leaner) model gives:\n\nelection_model_lean &lt;- lm(\n  LNP_Percent ~\n    BachelorAbv +\n    CurrentlyStudying +\n    DeFacto +\n    DiffAddress +\n    Finance + HighSchool +\n    Indigenous +\n    LFParticipation +\n    Married +\n    NoReligion,\n  data = election2022\n)\n\nsummary(election_model_lean)\n\n\nggplot(election2022, aes(x = BachelorAbv, y = LNP_Percent)) +\n  geom_point() +\n  geom_smooth() +\n  theme_minimal()\n\nggplot(election2022, aes(x = CurrentlyStudying, y = LNP_Percent)) +\n  geom_jitter() +\n  geom_smooth() +\n  theme_minimal()\n\nggplot(election2022, aes(x = DeFacto, y = LNP_Percent)) +\n  geom_jitter() +\n  geom_smooth() +\n  theme_minimal()\n\nggplot(election2022, aes(x = DiffAddress, y = LNP_Percent)) +\n  geom_jitter() +\n  geom_smooth() +\n  theme_minimal()\n\nggplot(election2022, aes(x = Finance, y = LNP_Percent)) +\n  geom_jitter() +\n  geom_smooth() +\n  theme_minimal()\n\nggplot(election2022, aes(x = HighSchool, y = LNP_Percent)) +\n  geom_jitter() +\n  geom_smooth() +\n  theme_minimal()\n\nggplot(election2022, aes(x = Indigenous, y = LNP_Percent)) +\n  geom_jitter() +\n  geom_smooth() +\n  theme_minimal()\n\nggplot(election2022, aes(x = LFParticipation, y = LNP_Percent)) +\n  geom_jitter() +\n  geom_smooth() +\n  theme_minimal()\n\nggplot(election2022, aes(x = Married, y = LNP_Percent)) +\n  geom_jitter() +\n  geom_smooth() +\n  theme_minimal()\n\nggplot(election2022, aes(x = NoReligion, y = LNP_Percent)) +\n  geom_jitter() +\n  geom_smooth() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne of the more novel issues with the above analysis is that it’s done on the electorate level, however electorates vary dramatically in side (google the Modifiable Area Unit Problem if this piques your interest)\nInstead of using one data point for a whole electorate (regardless of how big it is), we can fetch data from all 7,000 voting booths, the match it up with the corresponding local demographic data.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Australian election data</span>"
    ]
  },
  {
    "objectID": "13-Austrlaian-election-data.html#booth-data",
    "href": "13-Austrlaian-election-data.html#booth-data",
    "title": "Australian election data",
    "section": "Booth data",
    "text": "Booth data\nThe AEC maintains a handy spreadsheet of booth locations for recent federal elections. You can search for your local booth location (probably a school, church, or community center) in the table below.\n\n\n# A tibble: 10 × 15\n   State DivisionID DivisionNm PollingPlaceID PollingPlaceTypeID PollingPlaceNm \n   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;              &lt;dbl&gt; &lt;chr&gt;          \n 1 ACT          101 Canberra             8829                  1 Barton         \n 2 ACT          101 Canberra            64583                  5 Belconnen CANB…\n 3 ACT          101 Canberra            65504                  5 BLV Canberra P…\n 4 ACT          101 Canberra            11877                  1 Bonython       \n 5 ACT          101 Canberra             8802                  1 Braddon (Canbe…\n 6 ACT          101 Canberra            11452                  1 Calwell        \n 7 ACT          101 Canberra             8806                  1 Campbell       \n 8 ACT          101 Canberra             8761                  1 Chapman        \n 9 ACT          101 Canberra             8763                  1 Chisholm       \n10 ACT          101 Canberra             8808                  1 City (Canberra)\n# ℹ 9 more variables: PremisesNm &lt;chr&gt;, PremisesAddress1 &lt;chr&gt;,\n#   PremisesAddress2 &lt;chr&gt;, PremisesAddress3 &lt;chr&gt;, PremisesSuburb &lt;chr&gt;,\n#   PremisesStateAb &lt;chr&gt;, PremisesPostCode &lt;chr&gt;, Latitude &lt;dbl&gt;,\n#   Longitude &lt;dbl&gt;\n\n\nWhat do these booths look like on a map? Let’s reuse the CED map above and plot a point for each booth location.\n\nggplot() +\n  geom_sf(data = ced2021) +\n  geom_point(\n    data = booths, aes(x = Longitude, y = Latitude),\n    colour = \"purple\", size = 1, alpha = 0.3, inherit.aes = FALSE\n  ) +\n  labs(\n    title = \"Polling booths in Australia\",\n    subtitle = \" \",\n    caption = \"Data: Australian Electoral Commission 2016\",\n    x = \"\",\n    y = \"\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.ticks.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    legend.position = \"right\",\n    plot.title = element_text(size = 12),\n    plot.subtitle = element_text(size = 11),\n    plot.caption = element_text(size = 8)\n  ) +\n  xlim(c(112, 157)) +\n  ylim(c(-44, -11))\n\n\n\n\n\n\n\n\nFiguring out where a candidates votes come from within an electorate is fundamental to developing a campaign strategy. Even in small electorates (e.g. Wentworth), there are pockets of right leaning and left leaning districts. Once you factor in preference flows, this multi-variate calculus becomes important to winning or maintaining a seat.\nIn the eechidnapackage, election results are provided at the resolution of polling place. Unfortunately, these are yet to be updated for elections after 2016.\nThe data sets must be downloaded using the functions firstpref_pollingbooth_download, twoparty_pollingbooth_download or twocand_pollingbooth_download (depending on the vote type).\nThe two files need to be merged to be useful for analysis. Both have a unique ID for the polling place that can be used to match the records. The two party preferred vote, a measure of preference between only the Australian Labor Party (ALP) and the Liberal/National Coalition (LNP), is downloaded using twoparty_pollingbooth_download. The preferred party is the one with the higher percentage, and we use this to colour the points indicating polling places.\nWe see that within some big rural electorates (e.g. in Western NSW), there are pockets of ALP preference despite the seat going to the LNP. Note that this data set is on a tpp basis - so we can’t see the booths that were won by minor parties (although it would be fascinating).We see that within some big rural electorates (e.g. in Western NSW), there are pockets of ALP preference despite the seat going to the LNP. Note that this data set is on a tpp basis - so we can’t see the booths that were won by minor parties (although it would be fascinating).\n\n\n\n\n\n\n\n\n\nThe two candidate preferred vote (downloaded with twocand_pollingbooth_download) is a measure of preference between the two candidates who received the most votes through the division of preferences, where the winner has the higher percentage.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Australian election data</span>"
    ]
  },
  {
    "objectID": "13-Austrlaian-election-data.html#informal-votes",
    "href": "13-Austrlaian-election-data.html#informal-votes",
    "title": "Australian election data",
    "section": "Informal votes",
    "text": "Informal votes\nOver 700,000 people (around 5% of all votes cast) vote informally each election. Of these, over have ‘no clear first preference’, meaning their vote did not contribute to the campaign of any candidate.\nI’ll be honest, informal votes absolutely fascinate me. Not only are there 8 types of informal votes (you can read all about the Australian Electoral Commission’s analysis here), but the rate of informal voting varies a tremendous amount by electorate.\nBroadly, we can think of informal votes in two main buckets.\n\nProtest votes\nStuff-ups\n\nIf we want to get particular about it, I like to subcategorise these buckets into:\n\nProtest votes (i.e. a person that thinks they are voting against):\n\nthe democratic system,\ntheir local selection of candidates on the ballot, or\nthe two most likely candidates for PM.\n\nStuff ups (people who):\n\nfilled in the form wrong but a clear preference was still made\nstuffed up the form entirely and it didn’t contribute towards the tally for any candidtate\n\n\nThe AEC works tirelessly to reduce stuff-ups on ballot papers (clear instructions and UI etc), but there isn’t much of a solution for protest votes.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Australian election data</span>"
    ]
  },
  {
    "objectID": "14-Bayesian-for-the-common-man.html",
    "href": "14-Bayesian-for-the-common-man.html",
    "title": "Bayesian for the common man",
    "section": "",
    "text": "Enter Rev. Bayes\nNote: Chapter under development\nBayes’ theorem is named after 18th-century British statistician and theologian, Thomas Bayes. His remarkable (yet simple) theorem describes how to update the probability of a hypothesis based on new evidence.\nIn traditional frequentist statistics, probability is interpreted as the long-run frequency of events in repeated trials (sampling). A p-value from a sampling distribution tells us: What is the chance of seeing this result, given some hypothesis?\nBayesian statistics, on the other hand, allows us to incorporate prior knowledge or beliefs into our analysis and update those beliefs as we gather more data. The central question of Bayes’ Theorem therefore is: How likely is the hypothesis to be true, given the data I’ve seen?\nThis isn’t semantics - it’s a completely different way of seeing the world.\nWe can write Bayes’ theorem using probability notation.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bayesian for the common man</span>"
    ]
  },
  {
    "objectID": "14-Bayesian-for-the-common-man.html#enter-rev.-bayes",
    "href": "14-Bayesian-for-the-common-man.html#enter-rev.-bayes",
    "title": "Bayesian for the common man",
    "section": "",
    "text": "\\[\nP(B| A) = \\frac{P(A | B) P(B)}{P(A)}\n\\]\nWhere:\n\n\\(P(A | B)\\) is the posterior probability (updated belief about \\(A\\) after observing \\(B\\))\n\\(P(B | A)\\) is the likelihood (probability of observing \\(B\\), given that \\(A\\) is true)\n\\(P(A)\\) is the prior probability (initial belief about \\(A\\), before seeing \\(B\\))\n\\(P(B)\\) is the evidence (total probability of observing \\(B\\), across all possible values of \\(A\\))",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bayesian for the common man</span>"
    ]
  },
  {
    "objectID": "14-Bayesian-for-the-common-man.html#priors",
    "href": "14-Bayesian-for-the-common-man.html#priors",
    "title": "Bayesian for the common man",
    "section": "Priors",
    "text": "Priors\nTom Chivers, in his excellent book Everything Is Predictable, steps through the history of Bayes’ theorem.\nAs we can see in the denominator \\(P(A)\\), Bayes’ theorem relies on our initial belief about A before seeing B. This is implicit in the frequentists approach too - it’s just that they put equal possibility on each possible answer.\nThis creates a novel issue, known as Boole’s objection. Tom Chivers steps this through in his book:\n\nImagine… ’an urn filled with balls, either black or white. If you have a flat prior on the total number of black balls in the urn, then any given mix of black and white balls is equally likely. (If there are only four balls in the urn, you know have three possibilities - two black, one black and zero black - and they’re all equally likely.).\nBut if you assume that each ball is equally likely to be black or white - a flat prior on the probability of drawing a white or black each time - then your prior probability favours (very strongly if there are lots of balls) a roughly fifty-fifty mix in the urn as a whole.\n\nThe importance of priors becomes obvious when thinking about disease diagnosis. Let’s work through an example:",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bayesian for the common man</span>"
    ]
  },
  {
    "objectID": "15-causal-inference.html",
    "href": "15-causal-inference.html",
    "title": "Causal inference",
    "section": "",
    "text": "What causes what\nNote: Chapter under development\nCausal inference the process of determining cause-and-effect relationships. Simply finding an association between two variables might be suggestive of a causal effect… but it also might not.\nThe best book on this topic is Causal Inference: The Mixtape by the phenomenally talented Scott Cunningham.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Causal inference</span>"
    ]
  },
  {
    "objectID": "16-functions.html",
    "href": "16-functions.html",
    "title": "Functions",
    "section": "",
    "text": "Components\nNote: Chapter under development\nFunctions are set sof instructions that take inputs (arguments), processes them, and returns an output. Instead of writing the same code repeatedly, you can define a function once and use it whenever needed. This makes your scripts cleaner and easier to maintain.\nI recommend Hadley Wickham’s great chapter on functions in his book R for Data Science 2e.\nA function in R is defined using the function keyword. Here is a simple function that adds two numbers:\nadd_numbers &lt;- function(a, b) {\n  result &lt;- a + b\n  return(result)\n}\n\n# Using the function\nadd_numbers(3, 5)\nIn this example, we see a function consisting of:\nWe can also reference functions (from other packages) for use in our own custom functions. This data cleaning functions references dplyr, stringr, and tidyr.\n# Practical Example: Data Cleaning Function\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tidyr)\n\ndata_cleaner &lt;- function(df) {\n  df &lt;- df %&gt;%\n    dplyr::mutate(across(\n      where(is.character),\n      stringr::str_trim\n    )) %&gt;% # Trim whitespace\n    tidyr::drop_na() # Remove missing values\n  return(df)\n}\n\n# Example\ndf &lt;- data.frame(name = c(\"Alice    \", \" Bob\", NA), age = c(25, 30, 22))\ndata_cleaner(df)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "16-functions.html#components",
    "href": "16-functions.html#components",
    "title": "Functions",
    "section": "",
    "text": "A name (e.g., add_numbers)\nArguments (e.g., a, b)\nA body that performs calculations or operations\nA return statement (optional, but recommended)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "16-functions.html#luhn-example",
    "href": "16-functions.html#luhn-example",
    "title": "Functions",
    "section": "Luhn example",
    "text": "Luhn example\nIt turns out credit card numbers aren’t a random 16 digits, there’s an algorithm (named after it’s inventor Hans Peter Luhn) that determines the combinations possible.\nIt follows these steps:\n\nStarting from the right, double every second digit.\nIf doubling a digit results in a number greater than 9, subtract 9 from it.\nSum all the digits.\nIf the total sum is a multiple of 10, the number is valid; otherwise, it is invalid.\n\nThat’s a lot of info… instead let’s write a function to check if a credit card number is valid:\n\nvalidate_credit_card &lt;- function(card_number) {\n  digits &lt;- as.numeric(strsplit(as.character(card_number), \"\")[[1]])\n  n &lt;- length(digits)\n\n  # Double every second digit from the right\n  for (i in seq(n - 1, 1, by = -2)) {\n    digits[i] &lt;- digits[i] * 2\n    if (digits[i] &gt; 9) {\n      digits[i] &lt;- digits[i] - 9\n    }\n  }\n\n  # Check if the sum is a multiple of 10\n  return(sum(digits) %% 10 == 0)\n}\n\n# Example usage\nvalidate_credit_card(4532015112830366) # Should return TRUE\nvalidate_credit_card(1234567812345678) # Should return FALSE",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "16-functions.html#loops-in-functions",
    "href": "16-functions.html#loops-in-functions",
    "title": "Functions",
    "section": "Loops in functions",
    "text": "Loops in functions\nThe real power of functions comes when we introduce loops. Loops allow us to repeat a calculation for different parameter values. Factorials are a great example where a loop is needed.\nA factorial is a mathematical operation denoted by an exclamation mark (!), used to find the product of all positive integers less than or equal to a given number.\nLet’s write a looping function to calculate 10! (10x9x8x7x6x5x4x3x2x1)\n\n## Calculating Factorial Using a Loop\n\nfactorial_loop &lt;- function(n) {\n  if (n &lt; 0) {\n    stop(\"Factorial is not defined for negative numbers.\")\n  }\n  result &lt;- 1\n  for (i in 1:n) {\n    result &lt;- result * i\n  }\n  return(result)\n}\n\n# Example usage\nfactorial_loop(10) # Test 10!\n\nWe see factorials blow out in size very quickly. Try inputting 100! and see what happens.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "17-packages.html",
    "href": "17-packages.html",
    "title": "Packages",
    "section": "",
    "text": "Why packages exist\nThe “fundamental unit of shareable code” in R is a package. At its core, a package solves a simple problem: instead of copy-pasting functions between projects, you write the code once, bundle it properly, and load it anywhere with library().\nA package also enforces discipline. It requires documentation, consistent function signatures, and passing a battery of automated checks. Code that lives in a package is almost always better code than the same logic sitting in a loose .R script.\nThe best reference on this topic is R Packages (2e) by Hadley Wickham and Jennifer Bryan. This chapter walks through the key ideas using inflateR — a real package built for this book — as a worked example.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "17-packages.html#the-anatomy-of-a-package",
    "href": "17-packages.html#the-anatomy-of-a-package",
    "title": "Packages",
    "section": "The anatomy of a package",
    "text": "The anatomy of a package\nAn R package is a directory with a specific structure. The minimum required files are:\ninflateR/\n├── DESCRIPTION        # Package metadata\n├── NAMESPACE          # What the package exports\n├── R/                 # All R function files\n│   ├── adjust.R\n│   └── historical_value.R\n└── data/              # Bundled datasets (.rda files)\n    ├── uk_cpi.rda\n    ├── aud_cpi.rda\n    └── ...\nEverything else — tests, vignettes, data-raw scripts — is optional but recommended.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "17-packages.html#description",
    "href": "17-packages.html#description",
    "title": "Packages",
    "section": "DESCRIPTION",
    "text": "DESCRIPTION\nThe DESCRIPTION file is the identity card of your package. It tells R (and CRAN) who wrote it, what it does, and what it depends on.\nPackage: inflateR\nTitle: Inflation Adjustment for Historical Currency Values\nVersion: 0.1.1\nAuthors@R: person(\"Charles\", \"Coverdale\",\n                  email = \"charlesfcoverdale@gmail.com\",\n                  role = c(\"aut\", \"cre\"))\nDescription: Convert historical monetary values into their present-day\n    equivalents using bundled CPI data sourced from the World Bank\n    Development Indicators. Supports GBP, AUD, USD, EUR, CAD, JPY,\n    CNY, and CHF.\nLicense: MIT + file LICENSE\nURL: https://github.com/charlescoverdale/inflateR\nDepends: R (&gt;= 3.5.0)\nEncoding: UTF-8\nLanguage: en-US\nLazyData: true\nA few things worth noting:\n\nVersion follows major.minor.patch convention. CRAN submissions increment the version each time.\nLicense — MIT is the most permissive and widely used for open source R packages.\nLazyData: true — tells R to load bundled datasets on demand rather than at startup, keeping the package lightweight.\nDepends: R (&gt;= 3.5.0) — sets the minimum R version required. Don’t set this unnecessarily high.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "17-packages.html#writing-a-function",
    "href": "17-packages.html#writing-a-function",
    "title": "Packages",
    "section": "Writing a function",
    "text": "Writing a function\nAll functions live in .R files inside the R/ directory. Each file can hold one or many functions — the convention is to group related functions together.\nHere is the core function from inflateR, slightly simplified:\nadjust_inflation &lt;- function(amount, from_year, currency, to_year = NULL) {\n\n  # Look up currency code from country name if needed\n  country_lookup &lt;- c(\n    \"united kingdom\" = \"GBP\", \"australia\" = \"AUD\",\n    \"united states\"  = \"USD\", \"canada\"    = \"CAD\",\n    \"japan\"          = \"JPY\", \"china\"     = \"CNY\",\n    \"switzerland\"    = \"CHF\", \"germany\"   = \"EUR\"\n  )\n\n  lookup &lt;- country_lookup[tolower(trimws(currency))]\n  if (!is.na(lookup)) currency &lt;- lookup\n  currency &lt;- toupper(currency)\n\n  # Select the right CPI dataset\n  cpi_data &lt;- switch(currency,\n    GBP = uk_cpi, AUD = aud_cpi, USD = usd_cpi,\n    EUR = eur_cpi, CAD = cad_cpi, JPY = jpy_cpi,\n    CNY = cny_cpi, CHF = chf_cpi\n  )\n\n  # Default to_year to the latest available year\n  if (is.null(to_year)) {\n    to_year &lt;- min(as.integer(format(Sys.Date(), \"%Y\")), max(cpi_data$year))\n  }\n\n  # Calculate inflation adjustment\n  index_from &lt;- cpi_data$index[cpi_data$year == from_year]\n  index_to   &lt;- cpi_data$index[cpi_data$year == to_year]\n\n  round(amount * (index_to / index_from), 2)\n}\nThe formula is simple: multiply the original amount by the ratio of the two CPI index values. Everything else in the function is input validation and user convenience.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "17-packages.html#documenting-with-roxygen2",
    "href": "17-packages.html#documenting-with-roxygen2",
    "title": "Packages",
    "section": "Documenting with roxygen2",
    "text": "Documenting with roxygen2\nDocumentation in R packages is written as structured comments directly above each function, using the roxygen2 package. These comments get compiled into the .Rd help files that appear when you run ?adjust_inflation.\n#' Adjust a historical monetary value for inflation\n#'\n#' Converts an amount from a historical year into its equivalent value in a\n#' target year, using bundled CPI data from the World Bank.\n#'\n#' @param amount Numeric. The original monetary amount.\n#' @param from_year Integer. The year the amount is from.\n#' @param currency Character. A currency code (e.g. \"GBP\") or country\n#'   name (e.g. \"Australia\") — case-insensitive.\n#' @param to_year Integer. The target year. Defaults to the current year.\n#'\n#' @return A numeric value: the inflation-adjusted amount.\n#'\n#' @examples\n#' adjust_inflation(12, 1963, \"GBP\")\n#' adjust_inflation(50, 1980, \"AUD\", to_year = 2000)\n#'\n#' @export\nadjust_inflation &lt;- function(amount, from_year, currency, to_year = NULL) {\n  # ...\n}\nThe key tags are:\n\n\n\nTag\nPurpose\n\n\n\n\n@param\nDocuments each argument\n\n\n@return\nDescribes what the function returns\n\n\n@examples\nRunnable examples (CRAN will execute these)\n\n\n@export\nMakes the function available to users via library()\n\n\n\nAfter writing or editing roxygen2 comments, run devtools::document() to regenerate the help files and NAMESPACE.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "17-packages.html#the-namespace",
    "href": "17-packages.html#the-namespace",
    "title": "Packages",
    "section": "The NAMESPACE",
    "text": "The NAMESPACE\nThe NAMESPACE file controls what your package exposes to users and what it imports from other packages. You should never edit it by hand — devtools::document() generates it automatically from @export tags.\nFor inflateR, it looks like this:\n# Generated by roxygen2: do not edit by hand\n\nexport(adjust_inflation)\nexport(historical_value)\nOnly these two functions are visible to users. Any internal helper functions without @export are hidden.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "17-packages.html#bundling-data",
    "href": "17-packages.html#bundling-data",
    "title": "Packages",
    "section": "Bundling data",
    "text": "Bundling data\nOne of inflateR’s design decisions was to bundle CPI data inside the package rather than calling a live API at runtime. This makes the package work offline and in restricted environments.\nData is stored as compressed .rda files in the data/ directory. The workflow for creating them lives in data-raw/:\n# data-raw/cpi_data.R\nlibrary(WDI)\n\nraw &lt;- WDI(country = \"GB\", indicator = \"FP.CPI.TOTL\",\n           start = 1960, end = 2024)\n\nuk_cpi &lt;- raw[, c(\"year\", \"FP.CPI.TOTL\")]\nnames(uk_cpi) &lt;- c(\"year\", \"index\")\n\n# Rescale so 2020 = 100\nbase &lt;- uk_cpi$index[uk_cpi$year == 2020]\nuk_cpi$index &lt;- round((uk_cpi$index / base) * 100, 2)\n\nusethis::use_data(uk_cpi, overwrite = TRUE)\nThe data-raw/ folder is excluded from the built package (via .Rbuildignore) — it’s a reproducibility record, not something users need.\nEach dataset also needs a documentation file in R/data.R:\n#' UK CPI Data (1960-2024)\n#'\n#' Annual Consumer Price Index for the United Kingdom, sourced from the\n#' World Bank Development Indicators (FP.CPI.TOTL). Rescaled to 2020 = 100.\n#'\n#' @format A data frame with 65 rows and 2 columns:\n#' \\describe{\n#'   \\item{year}{Calendar year (integer)}\n#'   \\item{index}{CPI index value (numeric, base 2020 = 100)}\n#' }\n#' @source \\url{https://data.worldbank.org/indicator/FP.CPI.TOTL}\n\"uk_cpi\"",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "17-packages.html#checking-the-package",
    "href": "17-packages.html#checking-the-package",
    "title": "Packages",
    "section": "Checking the package",
    "text": "Checking the package\nBefore sharing or submitting a package, run a full check:\ndevtools::check()\nThis runs the same battery of tests that CRAN uses — checking documentation, examples, dependencies, file structure, and more. The goal is:\n0 errors | 0 warnings | 0 notes\nNotes are acceptable if they have a clear explanation (e.g. New submission on a first CRAN submission). Errors and warnings must be fixed.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "17-packages.html#installing-locally",
    "href": "17-packages.html#installing-locally",
    "title": "Packages",
    "section": "Installing locally",
    "text": "Installing locally\nDuring development, install the package from your local directory:\ndevtools::install()\nOr install directly from GitHub:\ndevtools::install_github(\"charlescoverdale/inflateR\")\nOnce installed, use it like any other package:\nlibrary(inflateR)\n\n# What is £12 from 1963 worth today?\nadjust_inflation(12, 1963, \"GBP\")\n#&gt; [1] 256.43\n\n# What would $1000 in 2020 have been worth in 1990?\nhistorical_value(1000, 1990, \"USD\", from_year = 2020)\n#&gt; [1] 504.80",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "17-packages.html#submitting-to-cran",
    "href": "17-packages.html#submitting-to-cran",
    "title": "Packages",
    "section": "Submitting to CRAN",
    "text": "Submitting to CRAN\nCRAN is the official R package repository — getting a package accepted means it’s permanently installable with install.packages() by anyone in the world.\nThe submission process:\n# 1. Run a final check\ndevtools::check()\n\n# 2. Submit\ndevtools::submit_cran()\nCRAN will run automated pre-tests on Windows and Debian, then a human reviewer will assess the package. First submissions typically take 2–5 business days.\nCommon rejection reasons: - Examples that take too long to run (keep them under a few seconds) - URLs in documentation that return errors - Missing or incorrectly formatted LICENSE file - Unexplained NOTEs in R CMD check\ninflateR is available on GitHub at github.com/charlescoverdale/inflateR and has been submitted to CRAN.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "18-UK-economic-data.html",
    "href": "18-UK-economic-data.html",
    "title": "UK Economic Data",
    "section": "",
    "text": "ONS\nNote: Chapter under development\nThe Office of National Statistics (ONS) collates and publishes most of the UK’s economic data sets.\nThere’s two main ways to access this data. Option 1, poorly formatted spreadsheets. Option 2, an API (which has been in beta for over 5 years). We’ll step through both.\nThere’s several R packages out there that either ping the ONS’ API or web scrape the data. Kostas Vasilopoulos has produced one of the better ones (and it’s on CRAN) called onsr.\nlibrary(onsr)\n\n# Get a list of all datasets available\nons_ids()\n\n [1] \"wellbeing-quarterly\"                          \n [2] \"wellbeing-local-authority\"                    \n [3] \"weekly-deaths-region\"                         \n [4] \"weekly-deaths-local-authority\"                \n [5] \"weekly-deaths-health-board\"                   \n [6] \"weekly-deaths-age-sex\"                        \n [7] \"uk-spending-on-cards\"                         \n [8] \"uk-business-by-enterprises-and-local-units\"   \n [9] \"traffic-camera-activity\"                      \n[10] \"trade\"                                        \n[11] \"tax-benefits-statistics\"                      \n[12] \"suicides-in-the-uk\"                           \n[13] \"sexual-orientation-by-region\"                 \n[14] \"sexual-orientation-by-age-and-sex\"            \n[15] \"retail-sales-index-large-and-small-businesses\"\n[16] \"retail-sales-index-all-businesses\"            \n[17] \"retail-sales-index\"                           \n[18] \"regional-gdp-by-year\"                         \n[19] \"regional-gdp-by-quarter\"                      \n[20] \"projections-older-people-sex-ratios\"          \n[21] \"projections-older-people-in-single-households\"\n[22] \"output-in-the-construction-industry\"          \n[23] \"online-job-advert-estimates\"                  \n[24] \"older-people-sex-ratios\"                      \n[25] \"older-people-net-internal-migration\"          \n[26] \"older-people-economic-activity\"               \n[27] \"mid-year-pop-est\"                             \n[28] \"life-expectancy-by-local-authority\"           \n[29] \"labour-market\"                                \n[30] \"index-private-housing-rental-prices\"          \n[31] \"house-prices-local-authority\"                 \n[32] \"health-accounts\"                              \n[33] \"gva-by-industry-by-local-authority\"           \n[34] \"generational-income\"                          \n[35] \"gdp-to-four-decimal-places\"                   \n[36] \"gdp-by-local-authority\"                       \n[37] \"faster-indicators-shipping-data\"              \n[38] \"cpih01\"                                       \n[39] \"childrens-wellbeing\"                          \n[40] \"ashe-tables-9-and-10\"                         \n[41] \"ashe-tables-7-and-8\"                          \n[42] \"ashe-tables-3\"                                \n[43] \"ashe-tables-27-and-28\"                        \n[44] \"ashe-tables-26\"                               \n[45] \"ashe-tables-25\"                               \n[46] \"ashe-tables-20\"                               \n[47] \"ashe-tables-11-and-12\"                        \n[48] \"ashe-table-5\"                                 \n[49] \"ageing-population-projections\"                \n[50] \"ageing-population-estimates\"                  \n[51] \"TS079\"                                        \n[52] \"TS078\"                                        \n[53] \"TS077\"                                        \n[54] \"TS076\"                                        \n[55] \"TS075\"                                        \n[56] \"TS074\"                                        \n[57] \"TS073\"                                        \n[58] \"TS072\"                                        \n[59] \"TS071\"                                        \n[60] \"TS070\"                                        \n\nons_datasets()\n\n# A tibble: 60 × 23\n   contacts     description      keywords id    last_updated links$editions$href\n   &lt;list&gt;       &lt;chr&gt;            &lt;list&gt;   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;              \n 1 &lt;df [1 × 3]&gt; Seasonally and … &lt;chr&gt;    well… 2023-12-13T… https://api.beta.o…\n 2 &lt;df [1 × 3]&gt; Estimates of li… &lt;chr&gt;    well… 2023-12-13T… https://api.beta.o…\n 3 &lt;df [1 × 3]&gt; Provisional cou… &lt;chr&gt;    week… 2026-02-25T… https://api.beta.o…\n 4 &lt;df [1 × 3]&gt; Provisional cou… &lt;chr&gt;    week… 2024-01-09T… https://api.beta.o…\n 5 &lt;df [1 × 3]&gt; Provisional cou… &lt;chr&gt;    week… 2024-01-09T… https://api.beta.o…\n 6 &lt;df [1 × 3]&gt; Provisional cou… &lt;chr&gt;    week… 2026-02-25T… https://api.beta.o…\n 7 &lt;df [1 × 3]&gt; These data seri… &lt;NULL&gt;   uk-s… 2024-05-16T… https://api.beta.o…\n 8 &lt;df [1 × 3]&gt; The data contai… &lt;chr&gt;    uk-b… 2022-11-03T… https://api.beta.o…\n 9 &lt;df [1 × 3]&gt; Experimental da… &lt;NULL&gt;   traf… 2024-06-20T… https://api.beta.o…\n10 &lt;df [1 × 3]&gt; Country by comm… &lt;chr&gt;    trade 2026-02-19T… https://api.beta.o…\n# ℹ 50 more rows\n# ℹ 20 more variables: links$latest_version &lt;df[,2]&gt;, $self &lt;df[,1]&gt;,\n#   $taxonomy &lt;df[,1]&gt;, methodologies &lt;list&gt;, national_statistic &lt;lgl&gt;,\n#   next_release &lt;chr&gt;, qmi &lt;tibble[,1]&gt;, related_datasets &lt;list&gt;,\n#   release_frequency &lt;chr&gt;, state &lt;chr&gt;, title &lt;chr&gt;, unit_of_measure &lt;chr&gt;,\n#   type &lt;chr&gt;, publications &lt;list&gt;, license &lt;chr&gt;, is_based_on &lt;df[,2]&gt;,\n#   canonical_topic &lt;chr&gt;, subtopics &lt;list&gt;, survey &lt;chr&gt;, …\n\nonsgdp &lt;- onsr::ons_get(id = \"gdp-to-four-decimal-places\")\n\nhead(onsgdp, 100)\n\n# A tibble: 100 × 7\n    v4_0 `mmm-yy` Time   `uk-only` Geography      `sic-unofficial`\n   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;           \n 1 102.  Dec-25   Dec-25 K02000001 United Kingdom K--N            \n 2  98.1 Dec-25   Dec-25 K02000001 United Kingdom R               \n 3  87.1 Dec-25   Dec-25 K02000001 United Kingdom B               \n 4 103.  Dec-25   Dec-25 K02000001 United Kingdom M               \n 5 107.  Dec-25   Dec-25 K02000001 United Kingdom N               \n 6 108.  Dec-25   Dec-25 K02000001 United Kingdom Q               \n 7 101.  Dec-25   Dec-25 K02000001 United Kingdom G-and-I         \n 8 116.  Dec-25   Dec-25 K02000001 United Kingdom T               \n 9 109.  Dec-25   Dec-25 K02000001 United Kingdom H-and-J         \n10 110.  Dec-25   Dec-25 K02000001 United Kingdom H               \n# ℹ 90 more rows\n# ℹ 1 more variable: UnofficialStandardIndustrialClassification &lt;chr&gt;",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>UK Economic Data</span>"
    ]
  },
  {
    "objectID": "18-UK-economic-data.html#nomis",
    "href": "18-UK-economic-data.html#nomis",
    "title": "UK Economic Data",
    "section": "Nomis",
    "text": "Nomis\nCensus data, as well as economic and social data sets that take into account location, are produced by the ONS’ Nomis service. Nomis has it’s very own R package called nomisr.\nThe last UK census was in 2021. While there is a census slated for 2031, there is a growing consensus that this may not be necessary, with new administrative data sets providing a more accurate picture of the UK’s population.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>UK Economic Data</span>"
    ]
  },
  {
    "objectID": "18-UK-economic-data.html#trade",
    "href": "18-UK-economic-data.html#trade",
    "title": "UK Economic Data",
    "section": "Trade",
    "text": "Trade\nIn April 2025, trade is well and truly the talk of the town.\nUK trade data is collated by HM Revenue & Customs (HMRC) and published on the website https://www.uktradeinfo.com/.\nPeter van der Meulen has developed a wonderful R package uktrade to query HMRC’s database and stitch together tables.\nHere’s a worked example:",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>UK Economic Data</span>"
    ]
  }
]